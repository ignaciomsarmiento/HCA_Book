[["index.html", "Haciendo Ciencia Abierta 1 Prefacio", " Haciendo Ciencia Abierta Ignacio Sarmiento Barbieri | Gustavo Castillo-Alvarez 2025-02-01 1 Prefacio La ciencia social debe ser accesible para una amplia interpretación crítica y contribución, promoviendo sociedades abiertas. Este libro complementa al taller “Haciendo Ciencia Abierta” que tiene como objetivo fortalecer las prácticas de investigación dentro de la comunidad hispanohablante. Sirve como un acompañamiento, proporcionando una guía detallada y recursos adicionales para los participantes. El Libro como el taller se centrará en tres áreas principales: Ética y Transparencia en el Diseño de Investigación: Aquí, se abordará la importancia de los planes de preanálisis, destacando cómo estos pueden contribuir a una mayor transparencia y confianza en los resultados de la investigación. Reproducibilidad y Flujos de Trabajo Reproducibles: En esta sección, se hará énfasis en el control de versiones mediante GitHub y la presentación efectiva de los resultados de investigación utilizando RMarkdown. La reproducibilidad es un pilar fundamental para asegurar que los hallazgos puedan ser verificados y utilizados como base para investigaciones futuras. Gestión Segura de Datos: Se destacarán las prácticas éticas de recopilación de datos y el archivado a largo plazo utilizando herramientas como OSF y Zenodo. La gestión adecuada de datos no solo protege la integridad de la investigación, sino que también garantiza que los datos estén disponibles para futuros investigadores de manera segura y organizada. Este libro está dirigido a estudiantes de posgrado, estudiantes de último año de pregrado y profesores de ciencias sociales en Colombia y América Latina. Queremos equipar a estos académicos con las herramientas y conocimientos necesarios para llevar a cabo investigaciones de alta calidad, que sean éticas, transparentes y reproducibles. En un mundo donde la información y la desinformación compiten por la atención pública, es crucial que la investigación en ciencias sociales mantenga altos estándares de rigor y apertura. Este libro pretende ser una guía para lograr estos objetivos, fomentando una comunidad de investigación que valore y practique la ciencia abierta. El contenido y enfoque de este libro están inspirados y adoptan materiales del texto Transparent and reproducible social science research: How to do open science] de Christensen, Garret; Freese, Jeremy; y Miguel, Edward, publicado por University of California Press. Este valioso recurso ha servido como fundamento para estructurar nuestro contenido y enriquecer nuestras discusiones sobre la importancia de la ciencia abierta. Agradecemos a la Berkeley Initiative for Transparency in the Social Sciences (BITSS), una iniciativa del Center for Effective Global Action (CEGA) de la Universidad de California, en Berkeley y a la Universidad de Los Andes por su apoyo y esperamos que los conocimientos y habilidades adquiridos aquí se extiendan más allá del aula, contribuyendo al avance de la investigación en nuestra región. "],["intro.html", " 2 Introducción 2.1 La Confiabilidad de la Evidencia 2.2 Cambios de la AEA sobre Transparencia y el Editor de Datos 2.3 Mejorar la Confiabilidad de la Investigación", " 2 Introducción El trabajo empírico, especialmente, la investigación en ciencias sociales, enfrenta una crisis de confianza debido a la falta de transparencia y casos de fraude. La toma de decisiones informadas en áreas como la salud, la economía y la política depende en gran medida de un trabajo de datos confiables. Sin embargo, la proliferación de casos de fraude y la falta de transparencia en la metodologías empíricas han generado una creciente desconfianza en los resultados obtenidos. Este libro tiene como objetivo abordar esta problemática mediante la implementación de herramientas y estrategias que mejoren la rigurosidad y la credibilidad en el trabajo empírico y especialmente en la investigación. 2.1 La Confiabilidad de la Evidencia ¿Cuán confiable es la evidencia actual que sustenta la toma de decisiones? Muchos creen que no es lo suficientemente confiable. Una crisis de confianza ha surgido en la investigación en ciencias sociales, con voces influyentes tanto dentro como fuera del ámbito académico señalando que la investigación relevante para la política es a menudo menos fiable de lo que se afirma, o incluso incorrecta. La creencia popular de que se pueden manipular las estadísticas para obtener cualquier respuesta deseada refleja esta pérdida de fe en la investigación, y la percepción de que muchos hallazgos científicos son simplemente abogacía disfrazada de ciencia. La exactitud y la credibilidad de la evidencia utilizada son extremadamente importantes y en este libro proporciona herramientas para aumentar el rigor y la credibilidad de el trabajo empírico. 2.1.1 Fraude En años recientes en la investigación han salido a la luz numerosos casos de fraude, quizás el caso más famoso recientemente es el de Francesca Gino donde se encontraron inconsistencias significativas en los datos presentados en sus publicaciones. Los responsables de dar a la luz con estas practicas cuestionadas fue el sitio web Data Colada que se dedica a promover la transparencia y la reproducibilidad en la investigación científica mediante la exposición de casos de mala praxis y fraude académico. Caso de Francesca Gino Francesca Gino, una destacada profesora de la Escuela de Negocios de Harvard, se vio envuelta en un escándalo de fraude académico que conmocionó al mundo de la investigación en ciencias sociales. Gino, conocida por su trabajo en comportamiento organizacional y psicología, fue acusada de manipular datos en varios estudios publicados en prestigiosas revistas académicas. Las acusaciones llevaron a una revisión exhaustiva de sus publicaciones, resultando en la retractación de varios artículos. Este escándalo no solo afectó la reputación de Gino, sino que también tuvo un impacto negativo en sus coautores y en la credibilidad de la institución que representaba. La Escuela de Negocios de Harvard se vio obligada a revisar sus procedimientos y políticas para prevenir futuros casos de mala conducta. Este caso subraya la importancia de la transparencia y la reproducibilidad en la investigación académica. La manipulación de datos no solo distorsiona la verdad científica, sino que también daña la confianza del público en la ciencia. La comunidad académica debe tomar medidas proactivas para garantizar la integridad de la investigación y protegerse contra el fraude. Puede ver mas del caso aqui Aunque el fraude es la excepción y no la regla, estos casos son síntomas dramáticos de problemas subyacentes más amplios. Al igual que en otros casos de fraude, las acciones de Gino fueron vistas como un síntoma de problemas más profundos en el sistema de investigación, donde las presiones para publicar y obtener reconocimiento pueden llevar a algunos académicos a cometer actos deshonestos. Es esencial fomentar una cultura de integridad y rigor científico para preservar la validez y la utilidad de la investigación en ciencias sociales. 2.1.2 Incapacidad para reproducir resultados Un segundo problema significativo en la practica empírica como en la investigación es la incapacidad de reproducir los resultados, lo cual es crucial para validar la fiabilidad de los hallazgos. Por ejemplo en economía el problema es particularmente importante donde muchos de los resultados son imposibles de reproducir. La siguiente tabla resume varios estudios que intentaron reproducir resultados de trabajos empíricos: Estudio Número de artículos (solicitudes) Reproducción intentada Reproducción exitosa Tasa de reproducción Tasa de reproducción por artículo empírico Dewald, Thursby, and Anderson (1986) antes del cambio de política 62 5 3 60.0% 4.8% Dewald, Thursby, and Anderson (1986) después del cambio de política 92 3 2 66.7% 2.2% McCullough and Vinod (2003) 193 62 14 22.6% 7.3% Chang and Li (2015) 67 59 29 49.2% 43.3% Dewald, Thursby, and Anderson (1986) intentaron reproducir trabajos publicados en el Journal of Money, Credit and Banking antes y después del cambio de política en la revista que solicitaba a los autores compartir los datos. Para los artículos antes del cambio , se intentó replicar 5 de 62 artículos, logrando éxito en 3 casos (60.0%). Después del cambio de política, la tasa de éxito aumentó ligeramente al 66.7%, aunque el número absoluto de reproducciones exitosas fue bajo.Por otro lado, McCullough and Vinod (2003) intentaron reproducir 62 de 193 artículos, con una tasa de éxito del 22.6%. Finalmente y mas recientemente, Chang and Li (2015) intentaron replicar 59 de 67 artículos, logrando éxito en 29 casos (49.2%). La capacidad de reproducir estudios es esencial para confirmar la validez de los resultados en ciencias sociales. Las tasas de reproducción varían significativamente, destacando la necesidad de políticas más robustas para mejorar la transparencia y reproducibilidad de los hallazgos. Estos problemas subrayan la importancia de fomentar una cultura académica que valore la reproducción y la transparencia, asegurando que las conclusiones sean fiables y puedan ser utilizadas de manera efectiva en la toma de decisiones. 2.2 Cambios de la AEA sobre Transparencia y el Editor de Datos En respuesta a los crecientes problemas de reproducibilidad y transparencia en la investigación en ciencias sociales, la Asociación Estadounidense de Economía (AEA, por sus siglas en inglés) ha implementado cambios significativos en sus políticas editoriales. Estos cambios están diseñados para mejorar la credibilidad y la rigurosidad de la investigación publicada en sus revistas. Una de las medidas clave ha sido la implementación de requisitos más estrictos para la divulgación de datos y métodos. Los autores ahora deben proporcionar acceso a los datos y a los códigos de análisis utilizados en sus estudios, permitiendo a otros investigadores verificar los resultados de manera independiente. Esta medida busca reducir el riesgo de errores y aumentar la transparencia en el proceso de investigación. Además de los requisitos de divulgación de datos, la AEA ha creado el puesto de Editor de Datos (Data Editor). Este rol es fundamental para garantizar que los datos y códigos proporcionados por los autores cumplan con los estándares de calidad y transparencia establecidos por la asociación. El Editor de Datos revisa los materiales de apoyo de los estudios aceptados para publicación, asegurándose de que sean completos, precisos y suficientes para permitir la replicación de los resultados. Este paso adicional en el proceso de revisión editorial busca fortalecer la confianza en los hallazgos publicados y promover una cultura de apertura y responsabilidad en la comunidad investigadora. Estos esfuerzos de la AEA representan un paso importante hacia la mejora de la integridad en la investigación económica. Al exigir la divulgación de datos y la revisión rigurosa de los mismos, la AEA no solo mejora la calidad de las investigaciones publicadas, sino que también fomenta un ambiente de mayor colaboración y verificación entre los investigadores. La esperanza es que estas iniciativas inspiren a otras organizaciones académicas a adoptar prácticas similares, contribuyendo a un avance significativo en la reproducibilidad y transparencia en todas las disciplinas científicas. 2.3 Mejorar la Confiabilidad de la Investigación Para prevenir la mala práctica en la investigación, es necesario fortalecer las normas para compartir datos, fomentar la transparencia y la reproducibilidad. Este libro tiene como objetivo abordar estos problemas y mejorar la confianza en el trabajo empírico y en la investigación. References Chang, Andrew C, and Phillip Li. 2015. “Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say’usually Not’.” Dewald, William G, Jerry G Thursby, and Richard G Anderson. 1986. “Replication in Empirical Economics: The Journal of Money, Credit and Banking Project.” The American Economic Review, 587–603. McCullough, Bruce D, and Hrishikesh D Vinod. 2003. “Verifying the Solution from a Nonlinear Solver: A Case Study.” American Economic Review 93 (3): 873–92. "],["inv_etica.html", " 3 Investigación Ética 3.1 Normas mertonianas para una investigación ética 3.2 Normas en la práctica 3.3 Problemas", " 3 Investigación Ética En este capítulo discutiremos el ethos de la investigación, es decir, los valores que deberían guiar la práctica científica. La ética en la investigación es fundamental para garantizar que el conocimiento generado sea confiable, válido y beneficioso para la sociedad. En este contexto, las normas mertonianas, propuestas por el sociólogo Robert K. Merton en 1942, representan uno de los marcos más influyentes y perdurables en la discusión sobre la ética en la investigación. Merton veía la ciencia como un sistema extremadamente eficiente para producir conocimiento, y su discusión sobre el ethos de la ciencia fue un esfuerzo por explicar por qué la ciencia funcionaba tan bien. Los investigadores han sostenido durante mucho tiempo que los valores de apertura y replicación son fundamentales para lo que hacen, pero en la práctica no siempre ha estado a la altura de esos ideales. Merton situaba a los científicos en un sistema social con un conjunto de normas y describía los incentivos que enfrentan los investigadores individuales al actuar dentro de esa estructura. Las normas tienen un carácter dual: los incentivos proporcionados por un sistema bien estructurado apoyan un comportamiento que se adhiere a las normas, pero el sistema también funciona porque los actores internalizan las normas: las aceptan. Los cuatro valores fundamentales de la investigación científica que Merton articula son universalismo, comunalidad, desinterés y escepticismo organizado. Las normas mertonianas proporcionan un marco ético esencial para la práctica de la ciencia, asegurando que la búsqueda del conocimiento se realice de manera justa, abierta y responsable. Es seguro decir que muchos (si no la mayoría) de los estudiantes de posgrado, donde nos incluimos, nunca reciben una formación formal en el ethos de la investigación científica que Merton discute. En la mayoría de los casos, los estudiantes simplemente asimilan los valores, expectativas y normas predominantes de los investigadores a través de su asesor, otros profesores y compañeros de estudio. Los aspirantes a científicos sociales a menudo simplemente absorben elementos del `ethos científico mientras interactúan con colegas, pero existe la preocupación de que también se puedan transmitir lecciones negativas de esta manera. Al adherirse a estos principios, los científicos contribuyen no solo al avance del conocimiento, sino también al bienestar y al progreso de la sociedad en su conjunto. A continuación, exploraremos las normas mertonianas para una investigación ética. 3.1 Normas mertonianas para una investigación ética 3.1.1 Universalismo El principio del universalismo es uno de los pilares fundamentales en la ética de la investigación científica según las normas mertonianas. Esta norma sostiene que las afirmaciones científicas deben evaluarse con criterios impersonales y universales, sin importar quién las haga. El conocimiento científico debe ser juzgado únicamente por su validez y mérito intrínseco, independientemente de la nacionalidad, género, religión, posición social o cualquier otra característica personal del científico. El universalismo es crucial porque promueve la objetividad y la equidad en la evaluación de los descubrimientos científicos. Al aplicar criterios universales, se asegura que el conocimiento científico sea accesible y evaluado de manera justa, evitando los prejuicios y la discriminación. Este principio fomenta un entorno en el que las ideas y los resultados pueden ser evaluados y aceptados basándose en evidencia empírica y razonamiento lógico, no en la autoridad o el prestigio del investigador. Una implicación importante es que las sociedades que promueven la igualdad de oportunidades educativas pueden experimentar un progreso científico más rápido. Cuando personas de diversos orígenes, independientemente de su género, etnia, religión, sexualidad o antecedentes académicos, pueden participar en la investigación, se enriquece el aprendizaje y el avance científico. Restringir el acceso a la formación científica excluye a grupos completos, empobreciendo el esfuerzo científico. A pesar de su importancia, la implementación del universalismo no está exenta de desafíos. Los prejuicios implícitos, las redes de poder y la desigualdad de recursos entre diferentes grupos y regiones pueden influir en la visibilidad y la aceptación de ciertos trabajos científicos. Además, la accesibilidad a recursos como financiamiento, equipos y formación avanzada puede variar significativamente, afectando la capacidad de algunos investigadores para contribuir al conocimiento científico global. 3.1.2 Comunalismo Este principio sostiene que los resultados del trabajo científico deben ser compartidos libremente con la comunidad científica y con la sociedad en general. En otras palabras, el conocimiento generado a través de la investigación científica no debe ser propiedad exclusiva del investigador, sino que debe ser un bien común accesible para todos. En otras palabras, y en marcado contraste con muchas otras formas de propiedad fuera de la investigación, el ethos científico exige que el conocimiento generado por la investigación pertenezca a toda la comunidad y no solo a quienes lo descubren. El comunalismo es vital porque promueve la transparencia, la colaboración y el progreso continuo en la ciencia. Al compartir los resultados y los datos de la investigación, se facilita la verificación de los hallazgos, la replicación de los estudios y la acumulación colectiva de conocimientos. Esta práctica también fomenta un ambiente de colaboración en lugar de competencia desleal, permitiendo a los científicos construir sobre el trabajo de otros y acelerar el avance científico. A pesar de sus beneficios, el comunalismo enfrenta varios desafíos. Entre ellos se encuentran la protección de la propiedad intelectual, ya que los investigadores y las instituciones a veces son reacios a compartir sus resultados debido a preocupaciones sobre la propiedad intelectual y los derechos de autor. Además, los costos de publicación en revistas de acceso abierto pueden ser elevados, lo que limita la capacidad de algunos investigadores para compartir sus hallazgos. Por último, la competencia académica y la presión por publicar y obtener financiación pueden llevar a algunos investigadores a ser menos abiertos con sus datos y resultados. 3.1.3 Desinterés La norma mertoniana del desinterés en la investigación ética establece que los científicos deben actuar con imparcialidad y objetividad, buscando el conocimiento por el bien común y no por beneficios personales o intereses privados. En otras palabras, los científicos deben estar motivados por la curiosidad intelectual y el deseo de contribuir al conocimiento colectivo, en lugar de por ganancias personales, fama o poder. Este principio implica también que el investigador ético debe informar los resultados tal como son, incluso si esto perjudica su reputación, contradice el conocimiento establecido o enfada a otras personas. El desinterés es crucial para mantener la integridad y la credibilidad de la ciencia. Cuando los científicos se adhieren a este principio, se reduce el riesgo de sesgos, manipulaciones de datos y conflictos de interés que pueden distorsionar los resultados y comprometer la calidad de la investigación. El desinterés fomenta la confianza pública en la ciencia, asegurando que los hallazgos y conclusiones sean genuinos y basados en evidencias sólidas. Sin embargo, a pesar de su importancia, la implementación del desinterés enfrenta varios desafíos. Los investigadores pueden enfrentar presiones externas de patrocinadores, instituciones académicas o la industria para obtener resultados favorables o que apoyen ciertos intereses. Además, la intensa competencia por financiación, publicaciones y reconocimiento puede tentar a algunos científicos a priorizar sus intereses personales sobre el bien común. Por último, no todos los conflictos de interés son fácilmente identificables o declarados, lo que puede comprometer la objetividad de la investigación. 3.1.4 Escepticismo Organizado Este principio establece que los científicos deben adoptar una actitud crítica y cuestionadora hacia las afirmaciones y teorías científicas, incluidas las propias. Una característica fundamental del enfoque de los investigadores científicos es que no deben aceptar las cosas al pie de la letra: necesitan ver pruebas. El escepticismo organizado implica someter todas las afirmaciones a un riguroso escrutinio y verificación, promoviendo la revisión por pares, la replicación de estudios y la evaluación continua de los resultados. Esto significa que los científicos no deben limitarse a temas socialmente aceptables ni a lo que las autoridades consideran adecuado estudiar: el ideal es examinar críticamente todo. A pesar de sus beneficios, el escepticismo organizado enfrenta varios desafíos. Los científicos pueden tener sesgos personales o estar influenciados por la opinión dominante en su campo, lo que puede dificultar la evaluación objetiva de nuevas ideas. Además, la presión por publicar rápidamente y obtener financiación puede llevar a algunos investigadores a no someter sus trabajos a un escrutinio suficientemente riguroso. Asimismo, las ideas verdaderamente innovadoras pueden ser rechazadas inicialmente por la comunidad científica debido a su novedad y a la dificultad de encajarlas en el paradigma existente. Estos desafíos resaltan la importancia de mantener un equilibrio entre el escepticismo y la apertura a nuevas perspectivas en la investigación científica. En última instancia, el escepticismo organizado no solo fortalece la calidad de la ciencia, sino que también fomenta un entorno en el que las ideas pueden ser rigurosamente probadas y evaluadas, garantizando que solo las teorías más robustas y bien fundamentadas sean aceptadas. Este enfoque crítico y abierto es esencial para el avance del conocimiento y para asegurar que la ciencia continúe siendo una herramienta confiable y efectiva para comprender el mundo. 3.2 Normas en la práctica Una camino para entender lo que piensa y hacen los investigadores es preguntarles directamente. Esto es lo que hicieron Anderson, Martinson, and De Vries (2007) en su trabajo titulado “Normative dissonance in science: Results from a national survey of US scientists”. En este trabajo investigaron la disonancia normativa en la ciencia, es decir, la discrepancia entre los ideales normativos ampliamente aceptados y las percepciones de los científicos sobre su propio comportamiento y el de otros. Para ello utilizando respuestas de una encuesta a 3,247 científicos en etapas iniciales y en la mitad de sus carreras, financiados por los Institutos Nacionales de Salud (NIH) de EE. UU. El NIH financia una amplia gama de investigadores, desde científicos de laboratorio en investigación biomédica hasta científicos sociales en muchas disciplinas cuyo trabajo trata temas de salud. Entonces, aunque no es una muestra completamente representativa de todos los académicos, sí cubre una gran variedad de campos. En este trabajo ademas de preguntarle sobre las cuatro normas de Merton, agrego dos valores adicionales: Gobernanza y Calidad. Al mismo tiempo emparejó cada uno con una “contranorma” que los académicos han identificado como existentes en la comunidad investigadora. Estas seis parejas de normas y contranormas se describen en la tabla 3.2.1. Por ejemplo, la contranorma del universalismo es el particularismo, que representa una falta de apertura a diferentes tipos de personas o investigadores, y específicamente la creencia de que la evidencia científica debe ser juzgada principalmente en función del historial de investigación del investigador en lugar de la calidad de la evidencia en sí. 3.2.1 Normas y Contranormas en la Ciencia Norma Descripción Contranorma Descripción Universalismo Los científicos evalúan la investigación solo en función de su mérito. Particularismo Los científicos evalúan el nuevo conocimiento y sus aplicaciones basándose en la reputación y productividad pasada del individuo o grupo de investigación. Comunalismo Los científicos comparten abiertamente sus hallazgos con colegas. Secretismo Los científicos protegen sus hallazgos más recientes para asegurar prioridad en la publicación, patente o aplicaciones. Desinterés Los científicos están motivados por el deseo de conocimiento y descubrimiento. Interés Los científicos compiten con otros en el mismo campo por financiamiento y reconocimiento de sus logros. Escepticismo Organizado Los científicos consideran todas las nuevas evidencias, hipótesis, teorías e innovaciones, incluso aquellas que desafían o contradicen su propio trabajo. Dogmatismo Organizado Los científicos invierten sus carreras en promover sus hallazgos, teorías o innovaciones más importantes. Gobernanza Los científicos son responsables de la dirección y control de la ciencia a través de la gobernanza, la autorregulación y la revisión por pares. Administración Los científicos dependen de los administradores para dirigir la empresa científica a través de decisiones de gestión. Calidad Los científicos juzgan las contribuciones de los demás a la ciencia principalmente en función de la calidad. Cantidad Los científicos evalúan el trabajo de los demás principalmente en función del número de publicaciones y subvenciones. Nota: Esta tabla fue adaptada de Anderson, Martinson, and De Vries (2007) La figura @ref(fig:anderson_fig) muestra los porcentajes de los científicos en dos etapas de su carrera (inicio y mitad) que tienen puntuaciones normativas y contranormativas en tres categorías: suscripción, comportamiento propio y comportamiento de otros. 3.2.2 Normas versus contranormas en la práctica Nota: Esta figura fue tomada de Anderson, Martinson, and De Vries (2007) Esta figura subraya la desconexión entre las normas a las que los científicos suscriben y lo que perciben como el comportamiento típico, especialmente en el comportamiento de otros científicos, lo que refleja la disonancia normativa en el entorno de investigación. En cuanto a la Suscripción a Normas (Subscription), la mayoría (aproximadamente 90%) de los científicos en la etapa de Mitad de Carrera suscriben a las normas (barra gris), con muy pocos teniendo puntuaciones iguales (rayada) o menores (negra) en comparación con las contranormas. Para los científicos en Inicio de Carrera, la situación es similar: una gran mayoría suscribe a las normas, con pocos teniendo puntuaciones iguales o menores en comparación con las contranormas. En términos de Comportamiento Propio (Own Behavior), aunque la mayoría de los científicos en Mitad de Carrera tienen puntuaciones normativas mayores que las contranormativas, hay una proporción significativa con puntuaciones iguales o menores en comparación con las contranormas. Este patrón se repite en los científicos en Inicio de Carrera, pero con una mayor proporción de científicos con puntuaciones iguales o menores en comparación con las contranormas. Respecto al Comportamiento de Otros (Others’ Behavior), una mayoría de los científicos en Mitad de Carrera perciben el comportamiento típico de otros científicos como más contranormativo que normativo. Esta percepción es aún más marcada entre los científicos en Inicio de Carrera, con una gran mayoría percibiendo el comportamiento de otros como más contranormativo que normativo. Existe una disonancia significativa entre los ideales normativos y las percepciones del comportamiento real, tanto propio como de otros, siendo más pronunciada en la percepción del comportamiento de otros científicos. Los científicos en ambientes más competitivos tienden a ver un comportamiento más contranormativo, lo cual crea una fuente persistente de tensión y estrés en la comunidad científica. Esta figura subraya la desconexión entre las normas a las que los científicos suscriben y lo que perciben como el comportamiento típico, especialmente en el comportamiento de otros científicos, lo que refleja la disonancia normativa en el entorno de investigación. Sin embargo, surge la pregunta de cuál parte de la figura deberíamos creer: la parte media, que es mixta pero en general apoya las normas, o la parte inferior, que presenta una visión pesimista del campo de la investigación en su conjunto. Es posible que la última sea demasiado pesimista. Tal vez todos escuchan sobre unos pocos “casos malos”, como los casos de fraude discutidos en el primer capítulo, y a partir de ahí condenan injustamente el estado de la ética en todo su campo. 3.3 Problemas En el trabajo empírico una preocupación importante son los falsos positivos, que como ilustra Ioannidis (2005) es un problema mucho más importante que lo que se había pensado previamente. El problema de falsos positivos está íntimamente ligado al sesgo de publicación y búsqueda de especificación. Un ejemplo humorístico, pero muy ilustrativo de esta problemática, es el cómic de XKCD titulado “Significant”. En este cómic, un grupo de investigadores prueba la hipótesis de que comer jellybeans causa acné, realizando múltiples pruebas con diferentes colores de jellybeans. Después de 20 pruebas, finalmente encuentran un color (verde) que muestra una asociación significativa con el acné, aunque esta asociación sea un falso positivo debido al simple azar. Este escenario refleja claramente el problema del sesgo de publicación y la búsqueda de especificación. El sesgo de publicación ocurre cuando algunos estudios son más propensos a ser publicados que otros en función de sus resultados. En el caso de los jellybeans, si consideramos que se realizaron 20 estudios separados, sólo se publicaría aquel que mostró un resultado significativo, ignorando los otros 19 que no encontraron ninguna asociación. La búsqueda de especificación, por otro lado, ocurre cuando los investigadores exploran múltiples formas de analizar los datos dentro de un mismo estudio, y sólo reportan aquellas que muestran resultados significativos. Si imaginamos que el investigador del cómic probó 20 formas diferentes de dividir la muestra hasta encontrar una que arrojara un resultado significativo, estaríamos frente a un caso de búsqueda de especificación. Estos dos fenómenos están estrechamente relacionados: la anticipación del sesgo de publicación por parte de los investigadores fomenta la búsqueda de especificación, ya que los investigadores se sienten presionados a encontrar resultados que sean “publicables”. John Ioannidis, en su influyente artículo “Why Most Published Research Findings Are False”, argumenta que debido a estos problemas, muchos de los hallazgos publicados en la literatura científica son falsos positivos. Según Ioannidis, la combinación de estudios con bajo poder estadístico, la flexibilidad en los diseños de estudio y los análisis, y el sesgo de publicación, contribuyen a una alta tasa de resultados falsos en la investigación científica. El cómic de XKCD sirve como una representación visual simple pero potente de estos problemas. Nos recuerda que debemos ser críticos con los resultados publicados y conscientes de los sesgos y prácticas que pueden distorsionalos. Al abordar estos problemas, es esencial mejorar la transparencia en la investigación, promover la publicación de todos los resultados, y adoptar prácticas estadísticas más rigurosas para reducir la prevalencia de falsos positivos y fortalecer la fiabilidad de nuestros hallazgos. Falsos Positivos en Tests de Hipótesis Una hipótesis (por ejemplo, “El aumento del salario mínimo está asociado con una disminución en la tasa de desempleo”) es verdadera o falsa en el mundo real. Debido a que el investigador no puede estudiar a todas las personas afectadas, debe probar la hipótesis en una muestra de esa población objetivo. No importa cuántos datos recoja el investigador, nunca podrá probar (o refutar) su hipótesis de manera absoluta. Siempre existirá la necesidad de hacer inferencias sobre los fenómenos en la población a partir de los eventos observados en la muestra (Banerjee et al. 2009). Los errores son inevitables hasta cierto punto en el proceso de tests de hipótesis debido a las limitaciones inherentes a la investigación y al análisis de datos. Por ejemplo, pensemos en la siguiente analogía, al igual que un juez en un juicio, el investigador debe tomar una decisión basada en la evidencia disponible. Un juez comienza presumiendo la inocencia del acusado y solo rechaza esta presunción si hay pruebas suficientes más allá de una duda razonable. De manera similar, el investigador empieza asumiendo la hipótesis nula (\\(H_0\\)). Utilizando pruebas estadísticas, el investigador debe determinar si hay suficiente evidencia para rechazar \\(H_0\\) en favor de la hipótesis alternativa (\\(H_1\\)). Aun así, siempre existe la posibilidad de error, condenando a un inocente (error de Tipo I) o liberando a un culpable (error de Tipo II), lo cual refleja la naturaleza incierta de la inferencia estadística. Podemos resumir este proceso en la siguiente tabla de contingencia: Decisión sobre \\(H_0\\) La hipótesis nula (\\(H_0\\)) es verdadera La hipótesis nula (\\(H_0\\)) es falsa No rechazar \\(H_0\\) Inferencia correcta (verdadero negativo)(probabilidad = \\(1−\\alpha\\)) Error de Tipo II (falso negativo)(probabilidad = \\(\\beta\\)) Rechazar \\(H_0\\) Error de Tipo I (falso positivo)(probabilidad = \\(\\alpha\\)) Inferencia correcta (verdadero positivo)(probabilidad = \\(1−\\beta\\)) En el contexto de los tests de hipótesis, un falso positivo ocurre cuando se rechaza incorrectamente la hipótesis nula (\\(H_0\\)) a favor de la hipótesis alternativa (\\(H_1\\)). En otras palabras, el test indica un resultado significativo cuando, en realidad, no lo es. Este error es conocido como error de Tipo I y se denota por \\(\\alpha\\), que representa la probabilidad de cometer un falso positivo. En otras palabras, si el test no lleva a No rechazar \\(H_0\\) puede ser que estemos: - Haciendo: Inferencia correcta (verdadero negativo) (probabilidad = \\(1−\\alpha\\)): La prueba correctamente no rechaza \\(H_0\\) cuando \\(H_0\\) es verdadera. Es la probabilidad de no cometer un error de Tipo I. - Cometiendo Error de Tipo II (falso negativo) (probabilidad = \\(\\beta\\)): La prueba no rechaza \\(H_0\\) cuando \\(H_1\\) es verdadera. Este es el error de Tipo II. En casos contrario, si Rechazamos \\(H_0\\) puede que estemos: - Cometiendo Error de Tipo I (falso positivo) (probabilidad = \\(\\alpha\\)): La prueba incorrectamente rechaza \\(H_0\\) cuando \\(H_0\\) es verdadera. Esta es la tasa de error de Tipo I. - Haciendo Inferencia correcta (verdadero positivo) (probabilidad = \\(1−\\beta\\)): La prueba correctamente rechaza \\(H_0\\) cuando \\(H_1\\) es verdadera. Esta es la potencia del test, que es 1 menos la probabilidad de cometer un error de Tipo II (\\(\\beta\\)). En resumen, los falsos positivos (\\(\\alpha\\)) son situaciones donde el test sugiere incorrectamente la existencia de 3.3.1 Modelo conceptual para aproximar estos problemas Un modelo muy útil para entender estos problemas claves fue desarrollado por Ioannidis (2005) en su artículo “Why Most Published Research Findings Are False”. Este artículo proporciona un marco teórico para entender por qué una gran proporción de los hallazgos científicos pueden ser falsos positivos. Este modelo se fundamenta en la estadística bayesiana y en varias consideraciones prácticas del proceso de investigación. Específicamente este modelo estima el valor predictivo positivo (positive predictive value o PPV) de la investigación. Esto es simplemente la probabilidad post-hoc de que un resultado sea verdadero condicional a que el test de hipótesis es significativo. Es decir \\[ PPV = Pr(V | S) \\] donde \\(V\\) indica que la hipótesis es verdadera, y S que el resultado del test estadístico es significativo, es decir, si \\(T\\) es el test estadístico observado y \\(\\tau_{\\alpha}\\) es el punto de corte relevante, tenemos que \\(T&gt;\\tau_{\\alpha}\\). Definamos \\(R\\) como la relación entre el número de “relaciones verdaderas” y “ninguna relación” entre las hipótesis comúnmente estudiadas en una disciplina. A partir de ella podemos definir que la probabilidad de que una relación verdadera para una dada hipótesis exista antes que un estudio sea llevado a cabo. \\[ P(V) = \\frac{R}{R+1} \\] podemos entonces usar el teorema de Bayes y la tabla de contingencia del recuadro anterior para derivar la \\(PPV\\) \\[ P(V | S) = \\frac{P(S | V) \\cdot P(V)}{P(S)} \\] Con esto tenemos que \\(P(S | V)\\), la probabilidad de obtener un resultado significativo si el hallazgo es verdadero, es \\(1 - \\beta\\) y que \\(P(S | F)\\), la probabilidad de obtener un resultado significativo si el hallazgo es falso, es \\(\\alpha\\). Con un poco de matematica obtenemos que \\[PPV = P(V | S) = \\frac{(1 - \\beta) \\cdot R}{(1 - \\beta) \\cdot R + \\alpha } \\] De la ecuación, se puede ver que cuanto mayor sea la potencia del estudio (mayor \\(1 - \\beta\\)) y más estricto sea el nivel de significancia estadística (menor \\(\\alpha\\)), más cercano estará el \\(PPV\\) a 1, eliminando en gran medida los falsos positivos. Al nivel de significancia usual de \\(\\alpha = 0.05\\) y en un estudio bien potenciado (\\(1 - \\beta = 0.80\\)) en una literatura donde se piensa que un tercio de todas las hipótesis son verdaderas ex ante (\\(R_i = 0.5\\)), el \\(PPV\\) es relativamente alto, alcanzando un 89 por ciento. Sin embargo, la realidad es considerablemente más complicada y, como describe Ioannidis (2005), podría llevar a tasas mucho más altas de falsos positivos en la práctica debido a la presencia de estudios con bajo poder estadístico, búsqueda de especificaciones, sesgo del investigador y el sesgo de publicación, Discutimos estas extensiones a continuación. Empezamos con el tema del poder estadístico. En economía empírica, Doucouliagos, Stanley, and Giles (2012), Ioannidis, Stanley, and Doucouliagos (2017), y otros han documentado que los estudios, en la práctica, tiene bajo poder estadístico. Por ejemplo, Ioannidis, Stanley, and Doucouliagos (2017) argumenta que el poder mediano de los estudios en economía es alrededor del 18% lo que implica que el PPV es de \\(64\\%\\), es decir que el \\(36\\%\\) de los resultados son falsos positivos. Más aun si se utilizan rutinariamente estudios con baja potencia para probar hipótesis que probablemente son falsas, la tasa de falsos positivos puede superar el 50 por ciento (por ejemplo, poder = 0.20 y \\(R_i = 0.2\\) da un \\(PPV = 0.44\\)). Este problema, y otros más, se ven exacerbadas si existe sesgo de publicación. Ioannidis (2005) extiende el modelo para incluir la posibilidad de sesgo del investigador, que puede tomar varias formas incluidas cualquier combinación de manipulación de datos, búsqueda de especificaciones, informes selectivos, e incluso fraude. Si incorporamos este sesgo como un termino \\(u\\), la expresión anterior queda \\[ PPV= \\frac{(1 - \\beta) \\cdot R + u \\beta R}{(1 - \\beta) \\cdot R + \\alpha + u\\beta R + u(1- \\alpha)} \\] Aquí, el número real de relaciones verdaderas (el numerador) permanece casi sin cambios, aunque hay un término adicional que captura los efectos del sesgo del investigador. Si volvemos al caso de \\(R_i = 0.5\\), y el nivel de significancia usual del 5 por ciento, pero ahora asumimos que el sesgo del autor es relativamente bajo, al 10 por ciento, el \\(PPV\\) cae del 83 por ciento al 65 por ciento. Si el 30 por ciento de los autores presentan sesgo en su presentación de resultados, el \\(PPV\\) cae dramáticamente al 49 por ciento, lo que significa que la mitad de los efectos significativos reportados son en realidad falsos positivos. La figura 3.3.1.1, obtenida de Ioannidis (2005), presenta una gama de valores de parámetros y el \\(PPV\\) resultante. El término \\(R_i\\) puede variar entre campos de investigación, donde las literaturas que están en una etapa más temprana y, por lo tanto, más exploratoria, presumiblemente tienen menores probabilidades de relaciones verdaderas. 3.3.1.1 Valor predictivo positivo para distintas comibnaciones de parámetros Nota: Esta figura fue tomada de Ioannidis (2005) Este marco simple resalta varios de los problemas de la investigación empírica, con el mismo Ioannidis (2005) concluyendo que la mayoría de los hallazgos publicados en medicina probablemente sean falsos. Esto es posible que también suceda en otras disciplinas aunque es difícil hacer una afirmación similar dada la dificultad de cuantificar algunos de los parámetros clave en el modelo. Sin embargo, este ejercicio plantea preocupaciones importantes sobre la fiabilidad de los hallazgos en muchas campos. Con este marco en mente, veamos evidencia empírica de múltiples campos que sugieren la presencia de sesgo de publicación y otros de los problemas anteriormente discutidos para luego presentar soluciones para resolverlos. 3.3.2 Evidencia del problema 3.3.2.1 Sesgos de publicación El sesgo de publicación surge si el resultado de un estudio influye sistemáticamente en su probabilidad de ser publicado. Usualmente, se espera que este sesgo funcione en contra de los estudios que no rechazan la hipótesis nula, ya que estos típicamente “serían” menos interesantes de ser publicados. Sin embargo, si como investigadores somos incapaces de rastrear el cuerpo completo de pruebas estadísticas que se han realizado, incluidas aquellas que no rechazan la nula (y, por lo tanto, son menos probables de ser publicadas), entonces no podemos determinar la verdadera proporción de pruebas en una literatura que rechazan la nula. Por lo tanto, es crucial entender cuántas pruebas y análisis se han realizado en toda la comunidad académica. Sterling (1959) en un breve artículo fue el primero en señalar el problema, advirtiendo que “cuando se usa un nivel fijo de significancia como criterio crítico para seleccionar que resultados difundir, puede resultar en resultados imprevistos.” Mas recientemente Franco, Malhotra, and Simonovits (2014) documentaron que una gran parte de los análisis empíricos en las ciencias sociales nunca se publican o ni siquiera se escriben, y que la probabilidad de que un hallazgo sea compartido con la comunidad en general disminuye drásticamente para los “hallazgos nulos” (es decir, hallazgos que no son estadísticamente significativos). Franco, Malhotra, and Simonovits (2014) examinaron los estudios que pasaron la revisión por pares y fueron incluidos en la encuesta TESS (time-sharing experiments for the social sciences) financiados por la National Science Foundation. TESS financió estudios en diversos campos de las ciencias sociales y Franco, Malhotra, and Simonovits (2014) rastrearon con éxito casi todos los estudios originales a lo largo del tiempo, registrando la naturaleza de los resultados empíricos así como la publicación final del estudio. La figura ?? resume los resultados hallados. 3.3.2.2 Mayoria de resultados nulos nunca son publicados Nota: Esta figura fue tomada de Mervis (2014) La figura muestra un patrón empírico sorprendente: los estudios donde la prueba principal de hipótesis arrojó resultados nulos tienen 40 puntos porcentuales menos de probabilidad de ser publicados en una revista que un resultado fuertemente significativo, y 60 puntos porcentuales menos de probabilidad de ser escritos. Este hallazgo tiene implicaciones potencialmente severas para nuestra comprensión de los hallazgos en cuerpos enteros de investigación en ciencias sociales, si los ceros nunca son vistos por otros académicos, incluso en forma de documento de trabajo. Esto implica que el \\(PPV\\) de la investigación es probablemente más bajo de lo que sería de otra manera, y también tiene implicaciones negativas para la validez de los meta-análisis, si los resultados nulos no son conocidos por los académicos que intentan sacar conclusiones más amplias sobre un cuerpo de evidencia. 3.3.2.3 Búsqueda de especificaciones Estrechamente ligado al problema de sesgo de publicación está el de búsqueda de especificación. La búsqueda de especificación se refiere a la flexibilidad en el análisis de datos que permite presentar casi cualquier resultado bajo un umbral arbitrario. Esta práctica puede llevar a que la significancia estadística pierda su sentido, ya que los investigadores pueden ajustar su metodología hasta obtener resultados que alcancen el umbral deseado. Andrew Gelman y Erik Loken (Gelman and Loken 2013) lo resumen de una forma muy elocuente Un conjunto de datos puede ser analizado de tantas formas diferentes (con las opciones que no son solo qué prueba estadística realizar sino también decisiones sobre qué datos incluir o excluir, qué medidas estudiar, qué interacciones considerar, etc.), que muy poca información es proporcionada cuando se afirma que un estudio obtuvo un resultado p &lt; .05. Esta práctica también es conocida como “p-hacking”, “specification searching”, “data-fishing”, grados de libertad del investigador, o “data-mining”. Estos términos reflejan la misma preocupación: la manipulación de los datos hasta que se obtienen resultados significativos. Es importante destacar que la búsqueda de especificación no implica necesariamente una intención maliciosa. Puede ser subconsciente o simplemente una práctica estándar del análisis estadístico (Gelman and Loken 2013). Los investigadores, a menudo sin darse cuenta, pueden probar múltiples enfoques analíticos y seleccionar aquellos que producen resultados estadísticamente significativos. Hay evidencia de comportamiento anómalo de pruebas estadísticas alrededor de umbrales arbitrarios, lo que sugiere la prevalencia de la búsqueda de especificación. Esto se puede observar en los gráficos que muestran acumulaciones inusuales de resultados justo por encima de los umbrales de significancia estándar, como el valor p de 0.05. 3.3.2.4 Estadisticos Z de revistas academicas top GerberPS Nota: Esta figura fueron tomads de A. Gerber, Malhotra, et al. (2008) y A. S. Gerber and Malhotra (2008) Estos gráficos ilustran cómo los resultados tienden a agruparse alrededor de los umbrales de significancia, lo que sugiere que los investigadores pueden estar ajustando sus análisis hasta alcanzar estos umbrales, en lugar de seguir una metodología predefinida y rigurosa. Es importante notar, que mientras que el sesgo de publicación implica una distorsión de un cuerpo de múltiples estudios de investigación, el sesgo también es posible dentro de un estudio dado. En las décadas de 1980 y 1990, el acceso a computadores mas potentes y nuevas bases de datos ha llevado a preocupaciones crecientes de que algunos investigadores estaban realizando un número cada vez mayor de análisis y reportando selectivamente aquellos análisis que apoyaban nociones preconcebidas, o que eran vistos como particularmente interesantes dentro de la comunidad investigadora, e ignorando, conscientemente o no, otras especificaciones que no lo hacían. Las preocupaciones sobre la búsqueda de especificaciones solo se han vuelto más prominentes en décadas mas recientes. En resumen, la búsqueda de especificación es una práctica preocupante en el análisis de datos que puede comprometer la validez de los resultados de investigación. Es fundamental que los investigadores reconozcamos este problema y adoptemos medidas para minimizar su impacto, como pre registrar estudios y mantener una transparencia total en los procesos analíticos. References Anderson, Melissa S, Brian C Martinson, and Raymond De Vries. 2007. “Normative Dissonance in Science: Results from a National Survey of US Scientists.” Journal of Empirical Research on Human Research Ethics 2 (4): 3–14. Banerjee, Amitav, UB Chitnis, SL Jadhav, JS Bhawalkar, and S Chaudhury. 2009. “Hypothesis Testing, Type i and Type II Errors.” Industrial Psychiatry Journal 18 (2): 127–31. Doucouliagos, Chris, Tom D Stanley, and Margaret Giles. 2012. “Are Estimates of the Value of a Statistical Life Exaggerated?” Journal of Health Economics 31 (1): 197–206. Franco, Annie, Neil Malhotra, and Gabor Simonovits. 2014. “Publication Bias in the Social Sciences: Unlocking the File Drawer.” Science 345 (6203): 1502–5. Gelman, Andrew, and Eric Loken. 2013. “The Garden of Forking Paths: Why Multiple Comparisons Can Be a Problem, Even When There Is No ‘Fishing Expedition’ or ‘p-Hacking’ and the Research Hypothesis Was Posited Ahead of Time.” Department of Statistics, Columbia University 348 (1-17): 3. Gerber, Alan S, and Neil Malhotra. 2008. “Publication Bias in Empirical Sociological Research: Do Arbitrary Significance Levels Distort Published Results?” Sociological Methods &amp; Research 37 (1): 3–30. Gerber, Alan, Neil Malhotra, et al. 2008. “Do Statistical Reporting Standards Affect What Is Published? Publication Bias in Two Leading Political Science Journals.” Quarterly Journal of Political Science 3 (3): 313–26. Ioannidis, John PA. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. Ioannidis, John PA, Tom D Stanley, and Hristos Doucouliagos. 2017. “The Power of Bias in Economics Research.” Oxford University Press Oxford, UK. Mervis, Jeffrey. 2014. “Why Null Results Rarely See the Light of Day.” American Association for the Advancement of Science. Sterling, Theodore D. 1959. “Publication Decisions and Their Possible Effects on Inferences Drawn from Tests of Significance—or Vice Versa.” Journal of the American Statistical Association 54 (285): 30–34. "],["preanalisis.html", " 4 Promoviendo Ciencia Transparante 4.1 Registo y Planes de Analisis Previos", " 4 Promoviendo Ciencia Transparante Sabemos que los investigadores tienen mucha flexibilidad en su investigación, y esto puede llevar a sesgos en estudios individuales y en literaturas de investigación enteras. Emplear toda la investigación sobre un tema es una forma importante de limitar los problemas notados en el capítulo anterior. La alternativa tradicional al meta-análisis es la revisión narrativa de la literatura, pero tiene importantes limitaciones. Por un lado, las revisiones de la literatura a menudo son bastante subjetivas en la práctica, lo que permite a diferentes académicos llegar a conclusiones divergentes incluso cuando se centran en la misma base de evidencia de investigación, aumentando las preocupaciones sobre el sesgo del investigador. Caracterizar sucintamente toda una literatura en forma narrativa también se vuelve difícil (si no imposible) a medida que crece el número de estudios subyacentes, haciendo que el análisis estadístico objetivo sea particularmente valioso. Pero, ¿cómo podemos saber que estamos capturando toda la investigación relevante al llevar a cabo un meta-análisis? Los registros de estudios son una forma importante para que se descubran investigaciones existentes sobre un tema, incluso si el trabajo no termina siendo publicado en una revista. 4.1 Registo y Planes de Analisis Previos Los investigadores clínicos en los Estados Unidos están obligados por ley desde 2007 a registrar prospectivamente los ensayos médicos en una base de datos pública y a publicar los resultados resumidos. Esto ayuda a crear un registro público de ensayos que de otro modo no se publicarían. También puede servir para la prespecificación con el fin de distinguir de manera más creíble la prueba de hipótesis de la generación de hipótesis. Los científicos sociales han comenzado a realizar estos registros y especificar planes de preanálisis completos, aunque su adopción ha sido mas lenta y no es obligatoria. 4.1.1 Registros de estudios El registro de estudios sirve como una forma útil de buscar hallazgos de investigación sobre un tema en particular y de lidiar potencialmente con el sesgo de publicación. La mayoría de los defensores del registro de estudios también promueven la preinscripción de estudios, incluyendo planes de pre-análisis (PAPs) que pueden ser publicados y sellados con una marca de tiempo incluso antes de que se recojan o estén disponibles los datos de análisis (Miguel et al. 2014). Un ejemplo de los beneficios potenciales de los registros de experimentos se presenta en Turner et al. (2008), que detalla las tasas de publicación de estudios relacionados con los ensayos clínicos de fase 2 y fase 3 para 12 antidepresivos aprobados por la Administración de Alimentos y Medicamentos de los Estados Unidos (FDA, por sus siglas en inglés) . La FDA toma una decisión interna sobre si los resultados del ensayo proporcionan evidencia de un impacto positivo del fármaco. Los autores recopilaron información sobre la decisión de la FDA para cada ensayo así como información sobre la publicación eventual del artículo a partir de una variedad de fuentes. La Figura 4.1.2 muestra las tasas drásticamente diferentes de publicación y el grado correspondientemente alto de sesgo de publicación. La figura muestra esencialmente todos los ensayos con resultados positivos se publicaron en revistas, aproximadamente el 50 por ciento de los estudios con resultados cuestionables se publicaron, y la mayoría de los estudios con resultados negativos (es decir, sin impacto positivo del fármaco) no se publicaron al menos cuatro años después de que el estudio se completó. En pocas palabras, la literatura publicada por sí sola proporciona una perspectiva altamente engañosa sobre la efectividad de estos fármacos. 4.1.2 Status de publicacion y decisión de la FDA #fig:turner Nota: Esta figura fue tomada de Turner et al. (2008) El registro es ahora la norma en la investigación médica para experimentos aleatorios, y los registros a menudo incluyen (o enlazan a) planes de análisis estadístico prospectivo como parte del protocolo del proyecto. 4.1.3 Planes de Analisis previos 4.1.3.1 ¿Qué incluir en un Planes de Análisis Previo (PAP)? Basándonos en la lista de verificación de la FDA, y las sugerencias de Christensen, Freese, and Miguel (2019) los PAPs en ciencias sociales deben considerar los siguientes 10 puntos: Diseño del Estudio. Indicar cuántos brazos tiene el estudio, si los tratamientos múltiples son mutuamente excluyentes o superpuestos, y cómo se realiza o estratifica exactamente la aleatorización. Si el estudio no es un experimento aleatorizado, debemos expecificar el método de diseño a utilizar (por ejemplo, controlando por observables, regresión discontinua, variables instrumentales, datos de panel con efectos fijos, diferencias en diferencias, etc.). Muestra del Estudio. ¿Quién será incluido y excluido del estudio? Los investigadores deben describir cómo se manejarán las variaciones de la muestra prevista (por ejemplo, deserción, incumplimiento con el tratamiento asignado, datos faltantes). Se deben proporcionar detalles sobre el uso de múltiples encuestas o fuentes de datos (por ejemplo, seguimientos repetidos) o múltiples años de datos administrativos. Medidas de Resultado. Definir las medidas de resultado del estudio y declarar cuáles son de importancia primaria y secundaria, basándose en los objetivos e hipótesis de investigación del estudio. Definir cuidadosamente estas medidas refiriéndose a variables específicas de la encuesta. Si se utiliza más de una pregunta de la encuesta para construir un resultado, indicar exactamente cómo se combinarán estas variables. Familias de Efectos. Si se planea tener multiples variables de resultados se debe considerar agrupar los resultados en familias. El PAP debe enumerar los conjuntos exactos de resultados que comprenden las familias, así como las preguntas de la encuesta u otras fuentes de datos que componen las variables y resultados. Ajuste de Pruebas de Múltiples Hipótesis. Debe especificarse y tenerse en cuenta explícitamente ajustando los valores p (p-values), ya sea controlando la tasa de error familiar (FWER) o la tasa de descubrimiento falso (FDR). El PAP debe incluir una descripción de cómo se hará esto, incluyendo exactamente qué hipótesis serán consideradas en el ajuste. Subgrupos. Los resultados a menudo se interrelacionan con las características basicas para probar efectos heterogéneos del tratamiento. Dado que esencialmente cada características que contiene un conjunto de datos podría usarse para evaluar estos efectos heterogéneos del tratamiento, independientemente de si hay o no una teoría razonable detrás de un efecto diferencial, los subgrupos son especialmente importantes para preespecificar en un PAP. Debemos enumerar cada subgrupo a ser probado, vinculándolos a preguntas específicas de la encuesta o variables. El problema de la multiplicidad de subgrupos también puede abordarse utilizando métodos FWER y FDR, y si este es el plan del investigador, el PAP debe indicarlo explícitamente. Dirección del Efecto. Una ventaja de escribir un PAP es el mayor poder de las pruebas estadísticas unilaterales. Siempre que la dirección, idealmente con alguna explicación del mecanismo o teoría del cambio, sea preespecificada, una prueba unidireccional es estadísticamente válida. Especificación Estadística Exacta. ¿Cuál es la especificación exacta que se utilizará en una prueba de hipótesis? Por ejemplo, si se va a realizar una regresión, el PAP debe indicar si la regresión es lineal, lineal generalizada (Poisson, binomial negativa, etc.) u otra forma; qué variables de control, efectos fijos, y demás serán incluidos; y cómo se calcularán los errores estándar (robustos a la heterocedasticidad, agrupados, bootstrap, método delta, u otros). En otras palabras, se debe presentar una ecuación estadística con una descripción explícita de los supuestos de distribución relevantes. Pero dada la flexibilidad que existe al elegir una especificación de regresión ex post, este aspecto del PAP es esencial. Modelo Estructural. Si está probando un modelo estructural, es decir, construyendo un modelo matemático formal desde los primeros principios de maximización de utilidad y estimando parámetros del modelo estructural, como a veces hacen los economistas, debe asegurarse de incluir una discusión detallada de su enfoque. La forma funcional de una función de utilidad o ecuación de maximización de beneficios está sujeta al menos a tanta flexibilidad post hoc como lo está la regresión estándar. ¡Sello que registre cuando fue publicado! Es importante que el plan pueda ser comprobado que fue realizado previo al estudio. Los planes de pre-análisis todavía son nuevos en las ciencias sociales, y esta lista probablemente evolucionará en los próximos años a medida que los investigadores exploren el potencial y las posibles limitaciones de esta nueva herramienta. Pero esta lista es un muy buen punto de partida. Para aquellos preocupados por la posibilidad de que se les anticipe en nuevos diseños de investigación y preguntas a partir de los PAPs o descripciones de proyectos publicados públicamente, varios de los registros de ciencias sociales actualmente permiten el embargo temporal de los detalles del proyecto. Por ejemplo, el registro de la AEA permite a un usuario ocultar ciertos detalles del estudio hasta el final del embargo. 4.1.3.2 PAPs y los Reportes Registrados Los Reportes Registrados (Registered Reports) son un formato de publicación que enfatiza la importancia de la pregunta de investigación y la calidad de la metodología al realizar la revisión por pares antes de la recopilación de datos. Los autores primero escriben un protocolo de estudio detallado en el PAP y luego lo envían a una revista antes que el experimento inicie o se recolecten los datos. La revista revisa el plan y da una aceptación en principio o no. 4.1.4 Modelos de publicacion: Standard y Regristrados Nota: Esta figura fue tomada de Christensen, Freese, and Miguel (2019) Este formato busca recompensar las preguntas y los diseños experimentales, a la vez que elimina una variedad de prácticas cuestionables de investigación, incluyendo el bajo poder estadístico, la búsqueda de especificaciones, el reporte selectivo de resultados y el sesgo de publicación, al tiempo que permite total flexibilidad para reportar hallazgos no esperados. Al mismo tiempo estos fomentan de la transparencia y la reproducibilidad, ya que al registrar públicamente sus planes de análisis, los investigadores mejoran la transparencia y la reproducibilidad de sus estudios. Otros científicos pueden revisar los planes registrados y comparar los resultados finales con lo que se había propuesto originalmente, promoviendo así la integridad científica. En economía el Journal of Development Economics (JDE) ha adoptado el uso de Registered Reports para mejorar la calidad y la transparencia de la investigación publicada en su revista. Mas detalles pueden ser vistos aquí References Christensen, Garret, Jeremy Freese, and Edward Miguel. 2019. Transparent and Reproducible Social Science Research: How to Do Open Science. University of California Press. Miguel, Edward, Colin Camerer, Katherine Casey, Joshua Cohen, Kevin M Esterling, Alan Gerber, Rachel Glennerster, et al. 2014. “Promoting Transparency in Social Science Research.” Science 343 (6166): 30–31. Turner, Erick H, Annette M Matthews, Eftihia Linardatos, Robert A Tell, and Robert Rosenthal. 2008. “Selective Publication of Antidepressant Trials and Its Influence on Apparent Efficacy.” New England Journal of Medicine 358 (3): 252–60. "],["reproducibilidad-y-estructura-de-proyectos.html", " 5 Reproducibilidad y Estructura de Proyectos 5.1 Flujos de Trabajo Reproducibles 5.2 Estructura de Proyectos 5.3 Gestión de Rutas en los Directorios de Trabajo 5.4 Nombrar archivos 5.5 Buenas prácticas para escribir código 5.6 Gestión de Paquetes Estadísticos", " 5 Reproducibilidad y Estructura de Proyectos La investigación se fortalece si sus resultados pueden ser replicados y confirmados por varios investigadores independientes. Cuando los investigadores emplean transparencia en su investigación, es decir, cuando documentan y comparten adecuadamente los datos y procesos asociados con sus análisis, la comunidad investigadora en general puede ahorrar tiempo valioso al reproducir o basarse en los resultados publicados. A menudo, los datos o el código de proyectos anteriores son reutilizados por nuevos investigadores para verificar hallazgos anteriores o desarrollar nuevos análisis. En este libro vamos a definir como investigación reproducible como el trabajo que puede ser recreado de manera independiente a partir de los mismos datos y el mismo código original. Replicabilidad por otro lado, es aplicar el mismo análisis a datos diferentes. La reproducibilidad es distinta de la replicabilidad. Es decir, un trabajo reproducible es aquel en el que los resultados se mantienen idénticos si los códigos o los datos no han cambiado. Reproducibilidad versus Replicabilidad Nota: Esta figura fue tomada de Arnold et al. (2019) 5.1 Flujos de Trabajo Reproducibles Reglas a recordar Escribe siempre código Automatiza todo lo que se puedas automatizar. Escribe un único script que ejecute todo el código de principio a fin. El trabajo con datos a menudo requiere miles o decenas de miles de líneas de código. Por lo tanto, para trabajar de manera transparente, debemos adoptar tanto las técnicas estadísticas y metodológicas reproducibles, como las prácticas de flujo de trabajo y software que hagan que nuestro trabajo sea fácilmente reproducible por otros. Al diseñar flujos de trabajo reproducibles y compartirlos con los diferentes componentes de nuestro proyecto de investigación, permitimos que otros desarrollen una comprensión profunda de nuestro trabajo. Esto permite revisar nuestros métodos, probar nuestro código, proponer cambios útiles y hacer contribuciones para desarrollar aún más nuestro proyecto. Los flujos de trabajo reproducibles facilitan enormemente el proceso de revisión por pares al permitir a los revisores el acceso a las diferentes partes de los proyectos necesarias para validar los resultados de la investigación. El objetivo de este capítulo es introducir las prácticas más importantes en este ámbito. Hay varias herramientas que los investigadores pueden utilizar para asegurarse de que su trabajo sea reproducible. A continuación, discutimos las prácticas de flujo de trabajo. Al usar estas herramientas, un investigador se acercará al ideal de reproducibilidad de “flujo de trabajo de un solo clic”, es decir, ser capaces de ejecutar todo el análisis desde cero con un solo clic . El código es solo un aspecto de una estructura más amplia que denominamos “flujo de trabajo”, que abarca la combinación de datos, código, organización y documentación. Todo, desde los nombres de archivos y variables hasta la organización de carpetas, el almacenamiento de datos y la programación eficiente y legible, forma parte del flujo de trabajo. Nota: Esta figura fue tomada de Arnold et al. (2019) Antes de ofrecer detalles específicos sobre cómo escribir código reproducible, es importante mencionar una regla fundamental: Escribe siempre código!!. Escribe código en lugar de trabajar manualmente en tu entorno de programación estadística preferido. Esto significa: No modificar datos manualmente, como con una hoja de cálculo. Es decir, evita usar Excel si es posible. No confiar en la línea de comandos, menús desplegables, u opciones de apuntar y hacer clic en software estadístico (como SPSS o Stata). En su lugar, haz absolutamente todo con scripts. La razón principal para esto es la reproducibilidad. Modificar datos en Excel o cualquier programa similar de hojas de cálculo no deja un registro de los cambios realizados ni una explicación del razonamiento o el momento detrás de cualquier modificación. Aunque pueda parecer fácil o rápido limpiar datos una sola vez en Excel, o hacer cambios “menores” para que los datos sean legibles por el software estadístico preferido del investigador, a menos que estos cambios se escriban en detalle minucioso, este enfoque simplemente no es reproducible por otros investigadores. Es mejor escribir un script de programación que importe los datos sin procesar, haga todos los cambios necesarios, con comentarios en el código que expliquen esos cambios, y guarde cualquier conjunto de datos intermedios utilizado en el análisis. Luego, un investigador puede compartir sus datos originales y el código, y otros investigadores pueden reproducir su trabajo exactamente. No olvides que ese otro investigador podrías ser tú en unos meses. Aunque entendemos que una buena cantidad de investigación se ha realizado usando menús desplegables en SPSS o Stata, desaconsejamos enfáticamente esta práctica. Como mínimo, si insistes en seguir esta ruta, utiliza las funciones de registro de comandos integradas en el software. En Stata, esto implica el comando cmdlog; en SPSS, implica usar el botón de pegar para agregar a un archivo de sintaxis. Al menos en ese caso, otros pueden ver los comandos que ejecutaste, incluso si no pueden reproducirlos fácilmente. El ideal de transparencia en la investigación es hacer todo, incluyendo cambios como redondeos y formateo, mediante scripts. Incluso la descarga de datos desde sitios web se puede hacer a través de un script. Por ejemplo, en R, la función download.file() se puede usar para guardar datos de un sitio web. En Stata, el comando copy hace lo mismo. Por supuesto, esto abre la posibilidad de que el archivo de datos cambie en línea. Finalmente, aquí hay dos principios generales de organización: Considera no guardar la salida estadística, sino solo guardar el código y los datos que la generan. Obviamente, esto sería irrealistamente tedioso para proyectos grandes (especialmente aquellos donde el análisis tarda días en ejecutarse), pero la idea es que deberías poder reproducir todos los pasos de tu análisis desde cero en cualquier momento. Siempre comenta el código, cuanto más comentes tu código, y cuanto más describas el contenido de cada archivo de script, mejor serán la posibilidades de reproducción. 5.2 Estructura de Proyectos Un punto de partida natural para describir un buen flujo de trabajo es detallar una estructura básica de carpetas en el computador, nuestra sugerencia inspirada en Gentzkow and Shapiro (2014) es la siguiente: Complex Project Folder Estructure/ ├── 01_build &lt;- construcción de los datos │ └── 01_input/ │ └── my_data.csv │ └── 02_scripts/ │ └── clean_data.R │ └── 03_output/ │ └── data_clean.csv │ └── 04_temp/ │ └── temp_merge.csv ├── 02_analysis &lt;- análisis de los datos │ └── 01_input/ │ └── data_clean.csv │ └── 02_scripts/ │ └── 01_regressions_table1.R │ └── 02_regressions_fig1.R │ └── 03_output/ │ └── 01_fig1.png │ └── 02_table1.tex │ └── 04_temp/ │ └── regressions.log └── 03_Document &lt;- Document └── Document.Rmd &lt;- script documento dinámico Por supuesto, los detalles variarán para diferentes tipos de investigación y los gustos de diferentes académicos, pero muchos proyectos pueden tener una estructura de archivos similar a la organización básica mostrada aquí. En primer lugar, necesita crear una carpeta maestra con un nombre corto pero descriptivo para el proyecto, un nombre que sea significativo tanto para usted como para sus colaboradores. 5.2.1 Creando la estructura de proyecto automáticamente Una opción para la creación de esta estructura es simplemente tener un script que lo hace por nosotros. Para ello utilizaremos un script en Bash que crea la estructura de carpetas descriptas anteriormente. #!/bin/bash # Crear directorios de nivel superior mkdir -p 01_build 02_analysis 03_document # Crear subdirectorios dentro de build y analysis for dir in 01_build 02_analysis do mkdir -p ./$dir/01_input mkdir -p ./$dir/02_scripts mkdir -p ./$dir/03_output mkdir -p ./$dir/04_temp done # Navegar al directorio 02_analysis/03_output y crear subdirectorios cd 02_analysis/03_output mkdir -p Figures Tables # Volver al directorio principal cd ../.. # Crear un archivo README.txt en blanco touch README.txt echo &quot;¡Carpetas creadas exitosamente!&quot; El script realiza 5 operaciones Shebang (#!/bin/bash): Indica que el script debe ejecutarse con Bash. Creación de directorios de nivel superior: mkdir -p 01_build 02_analysis 03_document crea tres carpetas principales: 01_build, 02_analysis, y 03_document. Creación de subdirectorios: Un bucle for recorre los directorios 01_build y 02_analysis, creando cuatro subcarpetas dentro de cada uno: 01_input, 02_scripts, 03_output, y 04_temp. Navegar y crear más subdirectorios: El script navega a 02_analysis/03_output y crea dos subcarpetas más: Figures y Tables. Creación de un archivo README.txt: El script regresa al directorio principal y crea un archivo en blanco llamado README.txt Mensaje de confirmación: echo \"¡Carpetas creadas exitosamente!\" imprime un mensaje indicando que el proceso se ha completado. Alternativamente, tambien puedes incluir texto en tu archivo de README.txt #!/bin/bash # Crear directorios de nivel superior mkdir -p 01_build 02_analysis 03_document # Crear subdirectorios dentro de build y analysis for dir in 01_build 02_analysis do mkdir -p ./$dir/01_input mkdir -p ./$dir/02_scripts mkdir -p ./$dir/03_output mkdir -p ./$dir/04_temp done # Navegar al directorio 02_analysis/03_output y crear subdirectorios cd 02_analysis/03_output mkdir -p Figures Tables # Volver al directorio principal cd ../.. # Crear el archivo README.txt con la estructura de carpetas y la descripción cat &lt;&lt;EOL &gt; README.txt Estructura de Carpetas: 01_build ├── 01_input ├── 02_scripts ├── 03_output └── 04_temp 02_analysis ├── 01_input ├── 02_scripts ├── 03_output │ ├── Figures │ └── Tables └── 04_temp 03_document Descripción de Carpetas: 01_build: - 01_input: Contiene archivos de entrada para el proceso de construcción. - 02_scripts: Contiene scripts utilizados para el proceso de construcción. - 03_output: Contiene los resultados del proceso de construcción. - 04_temp: Contiene archivos temporales generados durante el proceso de construcción. 02_analysis: - 01_input: Contiene archivos de entrada para el análisis. - 02_scripts: Contiene scripts utilizados para el análisis. - 03_output: Contiene los resultados del análisis. - Figures: Contiene figuras generadas por el análisis. - Tables: Contiene tablas generadas por el análisis. - 04_temp: Contiene archivos temporales generados durante el análisis. 03_document: - Contiene documentos relacionados con el proyecto. EOL echo &quot;¡Carpetas y README.txt creados exitosamente!&quot; 5.2.1.1 Cómo Ejecutar el Script en Windows Sí, puedes usar GitHub Desktop para instalar Git Bash y ejecutar scripts de Bash en Windows. Aquí están las instrucciones detalladas: Paso 1: Instalar GitHub Desktop. Para descargar e Instalar GitHub Desktop ve a desktop.github.com y descarga GitHub Desktop y sigue las instrucciones de instalación. Asegurarse que Git haya sido instalado, de caso contrario crearlo. Paso 2: Crear y guardar el Script. Para ello primero abre Notepad o cualquier editor de texto y luego copia el script anterior en el editor. Finalmente, guarda el archivo con la extensión .sh, por ejemplo, create_folders.sh. Paso 3: Ejecutar el Script Usando Git Bash: 3.1. Abre GitHub Desktop y crea un repositorio. En el explorador de Windows, busca la carpeta del repositorio. 3.2 Haz click derecho con el mouse para abrir Git Bash. 3.3. Una vez en la terminal ejecuta el archivo hacieno sh create_folders.sh Porque usamos Git Bash y no la terminal de windows? Preferimos Git Bash porque es una herramienta que proporciona un entorno similar a UNIX/Linux en Windows, ideal para desarrolladores que necesitan usar comandos de Git y scripts de Bash. Emula una terminal Bash con comandos y utilidades típicas de UNIX, lo que facilita la compatibilidad con herramientas y flujos de trabajo de desarrollo de sistemas basados en UNIX. Por otro lado, la terminal de Windows (Command Prompt) es el intérprete de comandos predeterminado en Windows, diseñado para ejecutar comandos nativos de Windows y scripts batch, siendo más adecuada para tareas administrativas y operaciones específicas del sistema operativo Windows. 5.2.1.2 Cómo Ejecutar el Script en Mac Paso 1: Crear y guardar el Script. Abre un editor de texto, copia el script y guárdalo con la extensión .sh, por ejemplo, create_folders.sh. Paso 2: Abrir la Terminal. Ve a Aplicaciones &gt; Utilidades &gt; Terminal. Paso 3: Navegar al Directorio del Script: 3.1. Usa el comando cd para ir al directorio donde guardaste el script. Por ejemplo: cd /Users/TuUsuario/PathToScript 3.2. Haz el script ejecutable, para ello ejecuta: chmod +x create_folders.sh 3.3. Finalmente, ejecuta el script haciendo ./create_folders.sh y las carpetas quedarán creadas en tu directorio. 5.3 Gestión de Rutas en los Directorios de Trabajo Muchos entornos de programación tienen un concepto poderoso de la “ruta de trabajo” o “directorio de trabajo”. Este es el lugar donde el software busca los archivos que le pides que cargue y donde guardará cualquier archivo que le pidas que guarde. Por ejemplo, en algunos entornos, la ruta de trabajo actual se muestra en la parte superior de la consola o ventana principal del software. Es importante siempre usar rutas de directorios relativas (como ./Data y no C:/Usuarios/Ignacio/Documentos/Proyecto/Datos) para que los usuarios en otras computadoras no tengan que renombrar cada referencia a un archivo y puedan en su lugar solo cambiar o asignar un directorio global una vez. En R puedes ver tu ruta completa ejecutando getwd(): getwd() #&gt; [1] &quot;/Usuarios/Ignacio/Documentos/Proyecto/Datos&quot; En esta sesión de R, el directorio de trabajo actual (piénsalo como “inicio”) está en la carpeta Documentos de Ignacio, en una subcarpeta llamada Datos Este código devolverá un resultado diferente cuando lo ejecutes, porque la estructura de directorios de tu computadora es diferente a la de Ignacio. Puedes establecer el directorio de trabajo desde R, pero NO lo recomendamos: setwd(&quot;/ruta/a/mi/ProyectoGenial&quot;) Establecer el directorio de trabajo usando setwd() puede parecer una solución sencilla, pero presenta varias desventajas importantes. Primero, afecta la reproducibilidad y la colaboración, ya que configura un directorio de trabajo específico que es local a tu máquina, lo que puede causar problemas cuando otros intenten ejecutar tu script en sus propias computadoras, debido a las diferentes estructuras de directorios. Usar rutas relativas, en cambio, asegura que el código sea más portátil y fácil de compartir. Además, herramientas como RStudio permiten crear proyectos que automáticamente establecen el directorio de trabajo al directorio del proyecto, eliminando la necesidad de setwd() y facilitando la organización y gestión del proyecto. Mantener el código libre de setwd() fomenta mejores prácticas de organización y reduce la probabilidad de conflictos, ya que establecer manualmente el directorio de trabajo puede llevar a errores difíciles de depurar, especialmente si se ejecutan múltiples scripts o se trabaja en diferentes proyectos simultáneamente. Finalmente, en flujos de trabajo automatizados o en entornos de producción, depender de setwd() puede ser problemático. Es preferible tener scripts que sean independientes de la configuración del directorio de trabajo y que funcionen correctamente sin necesidad de ajustes manuales. En resumen, evitar el uso de setwd() y utilizar rutas relativas mejora la reproducibilidad, facilita la colaboración y se alinea con las mejores prácticas de programación. Hay una mejor manera; una forma que también te pone en el camino para gestionar tu trabajo en R como un experto. Esa manera es el proyecto de RStudio. 5.3.1 Proyectos en RStudio Mantener todos los archivos asociados con un proyecto (datos de entrada, scripts de R, resultados analíticos y figuras) juntos en un directorio es una práctica tan común como sabia que RStudio ha incorporado esto a través de proyectos. Para crear un proyecto hay que hacer clic en Archivo &gt; Nuevo Proyecto y seguir los pasos mostrados en la siguiente figura. Figure 5.1: To create new project: (top) first click New Directory, then (middle) click New Project, then (bottom) fill in the directory (project) name, choose a good subdirectory for its home and click Create Project. Nota: Esta figura fue tomada de Wickham, Çetinkaya-Rundel, and Grolemund (2023) En este caso llamamos al proyecto r4ds. Una vez que completo este proceso, tendremos un nuevo proyecto de RStudio solo para este proyecto. Podemos verificar que el “inicio” de tu proyecto sea el directorio de trabajo actual: getwd() #&gt; [1] /Usuarios/Ignacio/Documents/r4ds Mas aún la carpeta asociada con tu proyecto observaremos el archivo .Rproj. Haciendo doble clic en ese archivo para reabrir el proyecto. Nota que vuelves a donde lo dejaste: es el mismo directorio de trabajo e historial de comandos, y todos los archivos en los que estabas trabajando siguen abiertos. Porque seguiste nuestras instrucciones anteriores, tendrás, sin embargo, un entorno completamente fresco, garantizando que comienzas con una pizarra limpia. Una vez que estés dentro de un proyecto, deberías usar siempre rutas relativas y no rutas absolutas. ¿Cuál es la diferencia? Una ruta relativa es relativa al directorio de trabajo, es decir, al directorio del proyecto. Las rutas absolutas apuntan al mismo lugar independientemente de tu directorio de trabajo. Se ven un poco diferentes dependiendo de tu sistema operativo. En Windows comienzan con una letra de unidad (por ejemplo, C:) o dos barras invertidas (por ejemplo, \\\\nombre_del_servidor) y en Mac/Linux comienzan con una barra “/” (por ejemplo, /usuarios/ignacio). Nunca deberías usar rutas absolutas en tus scripts, porque dificultan el compartir: nadie más tendrá exactamente la misma configuración de directorio que tú. Hay otra diferencia importante entre los sistemas operativos: cómo separas los componentes de la ruta. Mac y Linux usan barras inclinadas (por ejemplo, data/geiser.csv) y Windows usa barras invertidas (por ejemplo, data\\geiser.csv). R puede trabajar con cualquiera de los dos tipos (sin importar en qué plataforma estés usando), pero desafortunadamente, las barras invertidas tienen un significado especial para R, y para obtener una sola barra invertida en la ruta, necesitas escribir dos barras invertidas. ¡Eso hace que la vida sea frustrante, por lo que recomendamos siempre usar el estilo de Linux/Mac con barras inclinadas hacia adelante. 5.3.2 Rutas de Archivos y el Paquete {here} Una de las grandes frustraciones al compartir un proyecto de RStudio con un colaborador es que es común referirse a archivos con rutas de archivos absolutas que son específicas de tu computadora. Desafortunadamente, los archivos inevitablemente no estarán en el mismo lugar en la computadora de tu colaborador, y todas las referencias de ruta para cargar o escribir/guardar archivos no funcionarán. Esta frustración ha llevado a personas razonables a amenazar con incendiar la computadora y a proponer mejores enfoques. Nota: Esta figura fue tomada de Bryan 2017 Para tener un conjunto de rutas de archivos que funcionen en cualquier computadora que tenga una copia de tu Proyecto, necesitas usar el paquete {here}. Para instalarlo y ejecutarlo podemos hacer, install.packages(&quot;here&quot;) library(here) here() #&gt; [1] /Usuarios/Ignacio/Documents/r4ds Cuando ejecutamos la función here() esto nos devolverá la ruta a tu Proyecto actual. Esencialmente, determina dónde está el directorio de tu proyecto y proporciona esto como una base. Esto se puede usar para referenciar otras carpetas en tu proyecto de manera relativa. La función here() proporciona los detalles de la ruta del archivo hasta la carpeta de tu Proyecto de RStudio, y luego puedes agregar cualquier subcarpeta o detalles del nombre del archivo dentro de los paréntesis. Ahora, cuando compartas el Proyecto con un colaborador, las rutas de archivo para leer y escribir archivos funcionarán correctamente. 5.4 Nombrar archivos Cómo nombras archivos y directorios puede no parecer un punto importante, pero puede causar un gran dolor de cabeza si intentas usar código para automatizar procesos, y en el mejor de los casos, solo ralentiza las cosas. Hay una ciencia para elegir buenos nombres de archivos también. En general, es útil nombrar los archivos de código con un título corto y útil que describa su función, junto con un número al principio, para que el orden de ejecución de los archivos de código sea claro, como: 01-importar-limpiar_datos_redcap.R 02-importar-limpiar_datos_almacen.R 03-unir_datos_redcap_almacen.R Estos se ordenarán y organizarán fácilmente en tu carpeta, ya que los números al principio ayudan a organizarlos. También es útil numerar y nombrar claramente tus archivos de datos. Estos generalmente deben corresponder a los archivos de código que los crearon. Al final de un paso de código, puedes guardar/escribir tu archivo de datos resultante en un archivo *.rda o un archivo *.csv, para ser cargado al inicio del siguiente archivo de código. Estos podrían verse así: 01-datos_limpios_redcap.csv 02-datos_limpios_almacen.csv 03-datos_unidos_redcap_almacen.csv Observa que estamos utilizando un esquema de nombres ‘segmentados’, de modo que los archivos tienen un patrón de nombres consistente con segmentos en el mismo orden, y los segmentos distintos están separados por guiones bajos, mientras que los segmentos informativos compuestos por más de una palabra están separados por guiones. Es sorprendentemente importante ser reflexible sobre tu esquema de nombres segmentados desde el principio, ya que puede ayudarte de muchas maneras a organizar, ordenar y manipular archivos más adelante. Puede ser tentador nombrar tus archivos code.R o myscript.R, pero deberías pensar un poco más antes de elegir un nombre para tu archivo. Tres principios importantes para nombrar archivos son los siguientes: Debe ser legible por máquinas Debe ser legible por humanos Debe ordenarse bien en un directorio Buenos nombres de archivos son legibles por computadoras, legibles por humanos y funcionan bien con el orden predeterminado. Desglosamos estos tres criterios uno por uno. 5.4.1 Nombres de archivos legibles por computadora ¿Qué hace que un nombre de archivo sea legible por computadora? Primero, los archivos legibles por computadora no contienen espacios, puntuación ni caracteres especiales. Recuerda estas sabias palabras de Aaron Quinlan, un bioinformático, “un espacio en un nombre de archivo es un espacio en el alma de uno”. Son consistentes en el uso de mayúsculas, lo que significa que siempre sigues el mismo patrón de mayúsculas, ya sea todo en minúsculas, camel case (EstoEsLoQueQuieroDecirPorCamelCase) o cualquier otro. Finalmente, los buenos nombres de archivos hacen un uso deliberado de delimitadores de texto. El uso prudente de delimitadores facilita la búsqueda de patrones cuando buscas un archivo específico. Generalmente, se recomienda usar un guion bajo (_) para delimitar unidades de metadatos y un guion (-) para delimitar palabras dentro de una unidad de metadatos. Por ejemplo, aquí hay un buen nombre de archivo legible por computadora: 2024-06-17_ciencia-abierta_clase-01.pptx 5.4.2 Nombres de archivos legibles por humanos El nombre de archivo de ejemplo anterior no solo es legible por computadora, también es legible por humanos. Esto significa que un humano puede leer el nombre del archivo y tener una idea bastante buena de lo que hay en ese archivo. ¡Los buenos nombres de archivos son informativos! No debes tener miedo de usar nombres largos si eso es lo que se necesita para hacerlos descriptivos. 5.4.3 Nombres de archivos que funcionan bien con el orden predeterminado Si ordenas tus archivos alfabéticamente en una carpeta, quieres que se ordenen de una manera que tenga sentido. Ya sea que ordenes tus archivos por fecha o por un número secuencial, el número siempre va primero. Para las fechas, usa el formato AAAAMMDD, o tus archivos creados en abril de 1984 y 2020 estarán más cerca que los creados en marzo y abril de 2020. Si estás utilizando una numeración secuencial, agrega una cantidad sensata de ceros al frente según la cantidad de archivos de esa categoría que esperas tener en el futuro. 5.5 Buenas prácticas para escribir código Cada pieza de código que escribes tiene múltiples audiencias. La audiencia más importante es la computadora: si el código no entrega instrucciones inequívocas y correctas, el resultado no será el que pretendes. Pero el código también tiene otras audiencias. En algún momento, tú, tu coautor, tu asistente de investigación o alguien que desee reproducir tus hallazgos necesitará mirar el código para entenderlo o modificarlo. Un buen código se escribe teniendo en cuenta a todas estas audiencias. A continuación, recopilamos algunos de los principios más importantes que hemos aprendido sobre cómo escribir buen código. 5.5.1 Mantén el código modular Para garantizar la reproducibilidad en proyectos, es generalmente más efectivo utilizar múltiples archivos, cada uno encargado de una tarea específica. Esta estrategia ofrece varias ventajas importantes: Dividir el proyecto en múltiples scripts que realicen tareas específicas facilita la comprensión, el mantenimiento y la actualización del código. Cada script puede enfocarse en una parte del flujo de trabajo. Esto permite modificar o actualizar una parte del proyecto sin afectar a las demá. Con scripts modulares, es más fácil reutilizar código en diferentes proyectos o en diferentes etapas del mismo proyecto. Por ejemplo, un script que contiene una funcion puede ser reutilizado en múltiples partes del análisis】. En un entorno de trabajo colaborativo, tener scripts separados permite que diferentes miembros del equipo trabajen en diferentes partes del proyecto simultáneamente. Esto reduce los conflictos de código y facilita la integración de los cambios a través de sistemas de control de versiones como Git Es más sencillo depurar y probar scripts individuales que un único archivo monolítico. Puedes desarrollar pruebas unitarias para cada script, asegurándote de que cada parte del proceso funciona correctamente antes de integrar todo el flujo de trabajo. Por tanto, dividir un proyecto en múltiples archivos específicos para cada tarea no solo mejora la organización y mantenibilidad del código, sino que también facilita la colaboración y asegura una mejor reproducibilidad de los resultados. 5.5.2 Mantén el código corto y con propósito Ninguna línea de código debería tener más de 100 caracteres. Los scripts largos deben dividirse en funciones más pequeñas. Las funciones individuales no deberían tener normalmente más de 80 líneas. Los scripts no deberían ser más largos que unos pocos cientos de líneas. Si te resulta difícil hacer que un script largo sea corto y con propósito, es una señal de que necesitas reflexionar sobre la estructura lógica del directorio en su conjunto. Cada script y función debería tener un propósito claro e intuitivo. Parte el código y recuerda mantenerlo modular 5.5.3 Haz que tus funciones sean tímidas Muy relacionado con los dos puntos anteriores, es importante que las funciones sean timidas. Este principio muy utilizado en programación, (Make your functions shy) se refiere a que las funciones, y los scripts, deben ser pequeñas, específicas y limitadas en su alcance. El lector debe saber exactamente qué variables usa una función como entradas y qué variables puede potencialmente cambiar. La mayoría de las funciones deberían declarar explícitamente sus entradas y salidas y solo operar con variables locales. Es importante, que el conjunto de entradas y salidas sea lo más pequeño posible; las funciones deberían ser reacias a tocar más datos de los necesarios. Por ejemplo, si una función solo depende del parámetro beta, pásale solo beta y no todo el vector de parámetros. Usa variables globales raramente, si es que alguna vez lo haces. Esto es porque las funciones “tímidas” deben ocultar su complejidad interna y solo exponer lo necesario para su uso. Esto significa que deben tener una interfaz clara y sencilla, sin revelar detalles internos de su implementación. Además funciones pequeñas y específicas son más fáciles de reutilizar en diferentes partes del código y más fáciles de mantener y probar. Si una función es demasiado grande o hace demasiadas cosas, se vuelve más difícil de entender, depurar y modificar sin introducir errores. Al mismo tiempo.; as funciones “tímidas” tienden a depender menos de su contexto exterior, lo que significa que tienen menos efectos secundarios y son más predecibles. Esto facilita poder hacer pruebas y reduce la posibilidad de errores debido a cambios en otras partes del código. 5.5.4 Ordena tus funciones para una lectura lineal Un lector debería poder leer tu código de arriba hacia abajo sin tener que saltar de un lado a otro. Las subfunciones deben aparecer inmediatamente después de las funciones de nivel superior que las llaman. 5.5.5 Usa nombres descriptivos Los buenos nombres reemplazan los comentarios y hacen que el código se auto-documente. Por defecto, los nombres de variables, funciones, archivos, etc. deben consistir en palabras completas. Usa abreviaturas solo cuando estés seguro de que un lector no familiarizado con tu código las entendería y no haya ambigüedad. La mayoría de los economistas entendería que “income_percap” significa ingreso per cápita, por lo que no es necesario escribir income_percapita. Pero income_pc podría significar muchas cosas diferentes dependiendo del contexto. Abreviaturas como st, cnty y hhld están bien si se usan de manera consistente en todo el cuerpo del código. Pero usar blk_income para representar el ingreso en un bloque censal podría ser confuso. Evita tener múltiples objetos cuyos nombres no dejen claro cómo son diferentes: por ejemplo, scripts llamados state_level_analysis.do y state_level_analysisb.do o variables llamadas x y xx. Los nombres pueden ser más cortos o más abreviados cuando los objetos que representan se usan frecuentemente y/o muy cerca de donde se definen. Por ejemplo, a veces es útil definir nombres cortos para usar en cálculos algebraicos. Esto es difícil de leer: log_coefficient = log((income_percap&#39; * income_percap)^(-1) */// Esto es mejor: X = income_percap Y = log_wage log_coefficient = log((X&#39;*X)^(-1)*X&#39;*Y) 5.5.6 Sé consistente Hay muchos puntos de estilo de codificación que son en su mayoría una cuestión de gusto. Por ejemplo, a veces la gente escribe nombres de variables como hhld_annual_income y otras veces como hhldAnnualIncome. Aunque algunas personas tienen opiniones fuertes sobre cuál es mejor, no nos importa. Lo importante es que todos en un equipo usen convenciones consistentes. Esto es especialmente importante dentro de los scripts: si estás editando un programa en el que todos los scripts usan indentación de dos espacios, deberías usar también la indentación de dos espacios, incluso si eso rompe la regla normal. (O, deberías usar grep para actualizar el script a la indentación de cuatro espacios). 5.5.7 Comenta el código Nunca puedes comentar demasiado tu código. Los comentarios deben realmente explicar lo que hace el código en lugar de simplemente transliterar—es más útil describir x&lt;-1 con “inicializar el conteo de población a 1” que “establecer x igual a 1.” Los comentarios también deben ser revisados para asegurarse de que no transmitan información inexacta y no queden desactualizados. 5.5.8 Indenta el código La indentación del código es crucial en la programación por varias razones, independientemente del lenguaje de programación o entorno de desarrollo utilizado (Somos demasiado inteligentes para opinar sobre el debate de espacios vs. tabulaciones). La indentación adecuada hace que el código sea más fácil de leer y entender. Los desarrolladores pueden seguir la estructura lógica del código, lo que facilita la comprensión de su funcionamiento y la identificación de secciones específicas. Un código bien indentado reduce el esfuerzo necesario para revisar y mantener el código. La indentación ayuda a organizar el código en bloques lógicos, mostrando claramente dónde comienzan y terminan las estructuras de control como bucles, condiciones y funciones. Esto es especialmente útil en lenguajes donde la indentación es una parte sintáctica del lenguaje, como Python, pero también es beneficioso en otros lenguajes. Un código mal indentado puede llevar a errores difíciles de detectar. La estructura visual proporcionada por la indentación adecuada ayuda a evitar errores lógicos y de sintaxis, permitiendo a los desarrolladores ver más fácilmente las relaciones entre diferentes partes del código. 5.5.9 No dejes desorden No dejes desorden—elimina valores intermedios temporales o archivos necesarios. Mantener el código libre de valores y archivos temporales innecesarios mejora la legibilidad y la mantenibilidad. Un código desordenado con variables y archivos temporales puede confundir a otros investigadores o incluso al mismo autor al volver a revisar el código tiempo después. Usar prefijos como temp_ o x_ para identificar estos elementos o guardarlos en una carpeta dedicada ayuda a gestionarlos mejor y a eliminarlos cuando ya no son necesarios. No eliminar valores temporales o intermedios puede llevar a errores si se usan accidentalmente en lugar de las variables correctas. Eliminar estos valores una vez que han cumplido su propósito reduce el riesgo de errores lógicos y facilita la depuración del código En Stata, por ejemplo, se pueden utilizar las funcionalidades tempfile y tempvar para manejar archivos y variables temporales de manera eficiente. Estos comandos crean referencias temporales que son automáticamente eliminadas al final de la sesión o del script, ayudando a mantener el entorno de trabajo limpio y ordenado. tempfile tempdata save `tempdata&#39;, replace tempvar tempvar1 gen `tempvar1&#39; = var1 + var2 5.6 Gestión de Paquetes Estadísticos Es un error común suponer que los paquetes de software avanzados siempre producirán los mismos resultados exactos a través de diferentes versiones de software y plataformas. Sin embargo, a menudo esto no es así, especialmente con paquetes escritos por usuarios. (Para un ejemplo en economía que demuestra la importancia de este problema con la estimación numérica no lineal, ver McCullough and Vinod (2003)). Como mínimo, los investigadores siempre deben incluir información sobre qué versión del software utilizaron para ejecutar sus programas. La información sobre la versión del software se puede determinar usando el comando sessionInfo() en R, y los usuarios de Stata deben incluir el comando version en scripts para asegurar que los usuarios con versiones más nuevas de Stata usen los mismos algoritmos. Desafortunadamente, esto todavía no es garantía, ya que los algoritmos desactualizados o incorrectos no se reproducen en versiones más nuevas de Stata. Los usuarios de Python pueden usar el comando pip list o pip freeze para listar las versiones de todos los paquetes instalados, y crear un archivo requirements.txt para documentar las dependencias del entorno. La información sobre el procesador de la computadora y el sistema operativo también debería idealmente ser incluida, ya que estos pueden producir respuestas diferentes. Como mínimo, los investigadores siempre deben incluir información sobre qué versión del software utilizaron para ejecutar sus programas. La información sobre la versión del software se puede determinar usando el comando sessionInfo() en R, y los usuarios de Stata deben incluir el comando version en scripts para asegurar que los usuarios con versiones más nuevas de Stata usen los mismos algoritmos. Desafortunadamente, esto todavía no es garantía, ya que los algoritmos desactualizados o incorrectos no se reproducen en versiones más nuevas de Stata. La información sobre el procesador de la computadora y el sistema operativo también debería idealmente ser incluida, ya que estos pueden producir respuestas diferentes. Los usuarios de R pueden usar el comando packageVersion() para verificar las versiones de los paquetes, y pueden ejecutar versiones antiguas de paquetes archivadas en CRAN. Dado que R (y otros lenguajes de código abierto) requieren numerosos paquetes, registrar manualmente los paquetes no es recomendable. Para garantizar la reproducibilidad, los usuarios pueden utilizar el paquete renv, que permite gestionar entornos de proyectos y restaurar las versiones de los paquetes a un punto específico en el tiempo. Al usar renv, se crea un archivo renv.lock que registra las versiones exactas de los paquetes utilizados en el proyecto. Esto asegura que, al compartir el proyecto, otros usuarios puedan reproducir el entorno exacto y las versiones de los paquetes tal como existían en la fecha en que se escribió el código. Los usuarios de Stata pueden usar el comando viewsource para ver el código fuente de cualquier archivo .ado que utilicen. StataCorp implementa un sistema de control de versiones integrado que permite que los scripts y programas escritos en versiones antiguas continúen funcionando en versiones modernas del software. Esto significa que los investigadores pueden escribir un script y estar seguros de que se ejecutará correctamente en el futuro, independientemente de las actualizaciones del software. Para garantizar la reproducibilidad, es recomendable incluir el comando version al inicio de los scripts, especificando la versión de Stata en la que se escribió el código. Esto asegura que los comandos se ejecuten con los algoritmos de la versión especificada, preservando así la integridad de los resultados originales. Los usuarios de Python pueden crear entornos virtuales utilizando herramientas como venv o virtualenv para asegurar que las versiones de los paquetes y del propio Python sean consistentes con las usadas originalmente. Además, herramientas como pip freeze permiten generar un archivo requirements.txt que lista todas las versiones de los paquetes utilizados, lo cual facilita la recreación del entorno original en otro sistema. Otra opción es usar conda, que no solo maneja paquetes de Python sino también otras dependencias del sistema, proporcionando un entorno reproducible más robusto. 5.6.1 Palabras finales El movimiento hacia el software de código abierto como R y Python ha revolucionado el mundo de la programación. Estos lenguajes no solo son accesibles para todos, sino que también cuentan con comunidades dinámicas que impulsan su evolución constante. En este capítulo, exploramos la importancia de la reproducibilidad en la investigación y cómo un flujo de trabajo bien estructurado puede facilitar este objetivo. Discutimos herramientas y prácticas esenciales, como el uso de scripts automatizados, la creación de estructuras de proyectos organizadas y la gestión cuidadosa de las dependencias de software. Hemos destacado cómo herramientas específicas en R, Python y Stata pueden ayudar a mantener la consistencia y la reproducibilidad de los resultados, proporcionando ejemplos claros y prácticos. Sin embargo, entendemos que muchas disciplinas han usado durante mucho tiempo software propietario como SAS, SPSS y Stata. Aprender un nuevo lenguaje puede parecer una carga adicional para los investigadores. En este capítulo, ofrecimos sugerencias prácticas lo mas generales posibles, pero hicimos muchas referencias a comandos específicos tanto para R como para Stata, esperando agregar mas en el futuro, asegurando que sean útiles para el mayor número de investigadores. References Arnold, Becky, Louise Bowler, Sarah Gibson, Patricia Herterich, Rosie Higman, Anna Krystalli, Alexander Morley, Martin O’Reilly, Kirstie Whitaker, et al. 2019. “The Turing Way: A Handbook for Reproducible Data Science.” Zenodo. Gentzkow, Matthew, and Jesse M Shapiro. 2014. “Code and Data for the Social Sciences: A Practitioner’s Guide.” Chicago, IL: University of Chicago, 51. McCullough, Bruce D, and Hrishikesh D Vinod. 2003. “Verifying the Solution from a Nonlinear Solver: A Case Study.” American Economic Review 93 (3): 873–92. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\". "],["entornos-reproducibles.html", " 6 Entornos Reproducibles 6.1 Instalación de Miniconda 6.2 Gestión de Entornos 6.3 Gestión de Paquetes 6.4 Consejos Avanzados 6.5 Mejores Prácticas para Computación Científica 6.6 Conclusión 6.7 Recursos Adicionales", " 6 Entornos Reproducibles En la computación científica y la investigación, la reproducibilidad es crucial. Una de las principales herramientas que nos ayuda a lograr esto es conda, un potente sistema de gestión de paquetes y entornos. Este tutorial te guiará a través de la configuración y uso efectivo de conda en Windows, enfocándose en las mejores prácticas para la computación científica. 6.1 Instalación de Miniconda En lugar de instalar la distribución completa de Anaconda, usaremos Miniconda - una alternativa ligera que nos da justo lo que necesitamos. El método de instalación recomendado es a través de la interfaz de línea de comandos (CLI) para practicar estas habilidades esenciales. Los pasos de instalación siguen el tutorial oficial de CLI de la documentación oficial. 6.1.1 Pasos Abre Windows PowerShell y ejecuta estos comandos: wget &quot;https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe&quot; -outfile &quot;.\\miniconda.exe&quot; Start-Process -FilePath &quot;.\\miniconda.exe&quot; -ArgumentList &quot;/S&quot; -Wait del .\\miniconda.exe Una vez instalado, abre el Anaconda PowerShell Prompt desde tu Menú de Inicio Verifica tu instalación ejecutando: conda --version Deberías ver algo como: (base) PS C:\\Users\\USER&gt; conda --version conda 24.11.1 6.1.2 Entendiendo tu Prompt de Shell Esa línea (base) PS C:\\Users\\USER&gt; es el prompt de tu shell. Analicémosla: (base): Muestra tu entorno conda activo (base es el predeterminado) PS: Indica que estás en PowerShell C:\\Users\\USER&gt;: Muestra tu directorio actual 6.2 Gestión de Entornos Los entornos son espacios aislados donde puedes instalar versiones específicas de paquetes sin afectar otros proyectos. Así es como trabajar con ellos: ### Creando un Nuevo Entorno conda create --name mi_entorno python=3.11 numpy scipy matplotlib Esto crea un entorno llamado mi_entorno con Python 3.11 y algunos paquetes científicos esenciales: numpy, scipy y matplotlib. 6.2.1 Activación y Desactivación # Activar conda activate mi_entorno # Desactivar cuando hayas terminado conda deactivate Recuerda que el entorno activo se muestra en el prompt. Si hubiéramos activado mi_entorno el prompt se vería así: (mi_entorno) PS C:\\Users\\USER&gt; 6.2.2 Eliminando entornos Si deseas eliminar el entorno mi_entorno que creamos anteriormente, primero necesitas desactivarlo con conda deactivate. Después, usando el comando conda remove puedes eliminarlo: # Para eliminar el entorno y todos sus paquetes asociados conda remove --name mi_entorno --all 6.2.3 Comandos de Gestión de Entornos # Listar todos los entornos conda env list # Exportar configuración del entorno conda env export &gt; environment.yml # Recrear entorno desde archivo conda env create -f environment.yml 6.3 Gestión de Paquetes La gestión de paquetes en conda es sencilla: # Instalar paquetes conda install pandas seaborn # Eliminar paquetes conda remove seaborn # Actualizar paquetes conda update numpy # Buscar paquetes conda search scikit-learn 6.4 Consejos Avanzados 6.4.1 Usando mamba para Mayor Velocidad Si encuentras que conda es lento, prueba mamba - es una alternativa más rápida: # Instalar mamba conda install -n base -c conda-forge mamba # Usar mamba en lugar de conda mamba install pandas 6.4.2 Configurando conda-forge El canal conda-forge proporciona paquetes más actualizados: conda config --add channels conda-forge conda config --set channel_priority strict 6.5 Mejores Prácticas para Computación Científica Aunque en el curso profundizamos mucho más en herramientas y prácticas, aquí hay una breve descripción de recomendaciones fáciles de implementar de inmediato: Siempre usa archivos de entorno: Documenta tu entorno con environment.yml Usa versiones específicas: Especifica versiones exactas de paquetes críticos como matplotlib=1.4.3 Mantén los entornos mínimos: Solo instala lo que necesites Documenta todo: Incluye archivos README explicando la configuración de tu entorno Usa control de versiones: Rastrea tus archivos de entorno usando git y, cuando colabores, GitHub 6.6 Conclusión Usar conda efectivamente es clave para mantener flujos de trabajo reproducibles en computación científica. Siguiendo estas prácticas, tendrás un entorno robusto y reproducible para tu investigación. 6.7 Recursos Adicionales Documentación oficial de conda conda-forge Guía de instalación de Miniconda Nota: Este tutorial fue actualizado por última vez el 31 de enero de 2025. Por favor, consulta la documentación oficial para la información más reciente. "],["references.html", "References", " References Anderson, Melissa S, Brian C Martinson, and Raymond De Vries. 2007. “Normative Dissonance in Science: Results from a National Survey of US Scientists.” Journal of Empirical Research on Human Research Ethics 2 (4): 3–14. Arnold, Becky, Louise Bowler, Sarah Gibson, Patricia Herterich, Rosie Higman, Anna Krystalli, Alexander Morley, Martin O’Reilly, Kirstie Whitaker, et al. 2019. “The Turing Way: A Handbook for Reproducible Data Science.” Zenodo. Banerjee, Amitav, UB Chitnis, SL Jadhav, JS Bhawalkar, and S Chaudhury. 2009. “Hypothesis Testing, Type i and Type II Errors.” Industrial Psychiatry Journal 18 (2): 127–31. Chang, Andrew C, and Phillip Li. 2015. “Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say’usually Not’.” Christensen, Garret, Jeremy Freese, and Edward Miguel. 2019. Transparent and Reproducible Social Science Research: How to Do Open Science. University of California Press. Dewald, William G, Jerry G Thursby, and Richard G Anderson. 1986. “Replication in Empirical Economics: The Journal of Money, Credit and Banking Project.” The American Economic Review, 587–603. Doucouliagos, Chris, Tom D Stanley, and Margaret Giles. 2012. “Are Estimates of the Value of a Statistical Life Exaggerated?” Journal of Health Economics 31 (1): 197–206. Franco, Annie, Neil Malhotra, and Gabor Simonovits. 2014. “Publication Bias in the Social Sciences: Unlocking the File Drawer.” Science 345 (6203): 1502–5. Gelman, Andrew, and Eric Loken. 2013. “The Garden of Forking Paths: Why Multiple Comparisons Can Be a Problem, Even When There Is No ‘Fishing Expedition’ or ‘p-Hacking’ and the Research Hypothesis Was Posited Ahead of Time.” Department of Statistics, Columbia University 348 (1-17): 3. Gentzkow, Matthew, and Jesse M Shapiro. 2014. “Code and Data for the Social Sciences: A Practitioner’s Guide.” Chicago, IL: University of Chicago, 51. Gerber, Alan S, and Neil Malhotra. 2008. “Publication Bias in Empirical Sociological Research: Do Arbitrary Significance Levels Distort Published Results?” Sociological Methods &amp; Research 37 (1): 3–30. Gerber, Alan, Neil Malhotra, et al. 2008. “Do Statistical Reporting Standards Affect What Is Published? Publication Bias in Two Leading Political Science Journals.” Quarterly Journal of Political Science 3 (3): 313–26. Ioannidis, John PA. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. Ioannidis, John PA, Tom D Stanley, and Hristos Doucouliagos. 2017. “The Power of Bias in Economics Research.” Oxford University Press Oxford, UK. McCullough, Bruce D, and Hrishikesh D Vinod. 2003. “Verifying the Solution from a Nonlinear Solver: A Case Study.” American Economic Review 93 (3): 873–92. Mervis, Jeffrey. 2014. “Why Null Results Rarely See the Light of Day.” American Association for the Advancement of Science. Miguel, Edward, Colin Camerer, Katherine Casey, Joshua Cohen, Kevin M Esterling, Alan Gerber, Rachel Glennerster, et al. 2014. “Promoting Transparency in Social Science Research.” Science 343 (6166): 30–31. Sterling, Theodore D. 1959. “Publication Decisions and Their Possible Effects on Inferences Drawn from Tests of Significance—or Vice Versa.” Journal of the American Statistical Association 54 (285): 30–34. Turner, Erick H, Annette M Matthews, Eftihia Linardatos, Robert A Tell, and Robert Rosenthal. 2008. “Selective Publication of Antidepressant Trials and Its Influence on Apparent Efficacy.” New England Journal of Medicine 358 (3): 252–60. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\". "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
